{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This script parses .html files previously downloaded into local folders for those schools (or organizations, generally) listed in a .csv directory file. It uses the BeautifulSoup, multiprocessing, and pandas modules to efficiently clean, filter, and merge webtext into various lists; it also uses dictionary methods to count the number of times any word from the provided dictionaries (essentialist and progressivist school ideologies, in this case) occurs in any page for a given school. The script then stores these lists to each school's folder as text files; incorporates them into a large pandas DataFrame; and then finally saves this as an analysis-ready pickle-formatted file.\\n\\nAuthor: Jaren Haber, PhD Candidate in UC Berkeley Sociology. \\nDate: January 7th, 2018.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8\n",
    "\n",
    "# # Parsing & Categorizing HTML from `wget` run with multiprocessing\n",
    "\n",
    "\n",
    "\"\"\"This script parses .html files previously downloaded into local folders for those schools (or organizations, generally) listed in a .csv directory file. It uses the BeautifulSoup, multiprocessing, and pandas modules to efficiently clean, filter, and merge webtext into various lists; it also uses dictionary methods to count the number of times any word from the provided dictionaries (essentialist and progressivist school ideologies, in this case) occurs in any page for a given school. The script then stores these lists to each school's folder as text files; incorporates them into a large pandas DataFrame; and then finally saves this as an analysis-ready pickle-formatted file.\n",
    "\n",
    "Author: Jaren Haber, PhD Candidate in UC Berkeley Sociology. \n",
    "Date: January 7th, 2018.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Initializing\n",
    "\n",
    "# import necessary libraries\n",
    "import os, re, fnmatch # for navigating file trees and working with strings\n",
    "import csv # for reading in CSV files\n",
    "#from glob import glob,iglob # for finding files within nested folders--compare with os.walk\n",
    "import json, pickle, csv # For saving a loading dictionaries, DataFrames, lists, etc. in JSON, pickle, and CSV formats\n",
    "from datetime import datetime # For timestamping files\n",
    "import time, timeout_decorator # To prevent troublesome files from bottlenecking the parsing process, use timeouts\n",
    "import sys # For working with user input\n",
    "import logging # for logging output, to help with troubleshooting\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words\n",
    "stemmer = PorterStemmer()\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "import urllib, urllib.request # for testing pages\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "from multiprocessing import Pool # for multiprocessing, to increase parsing speed\n",
    "import pandas as pd # modifies data more efficiently than with a list of dicts\n",
    "from tqdm import tqdm # For progress information over iterations, including with Pandas operations via \"progress_apply\"\n",
    "\n",
    "# Import parser\n",
    "from bs4 import BeautifulSoup # BS reads and parses even poorly/unreliably coded HTML \n",
    "from bs4.element import Comment # helps with detecting inline/junk tags when parsing with BS\n",
    "import lxml # for fast HTML parsing with BS\n",
    "bsparser = \"lxml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set script options\n",
    "\n",
    "Debug = False # Set to \"True\" for extra progress reports while algorithms run\n",
    "notebook = True # Use different file paths depending on whether files are being accessed from shell (False) or within a Jupyter notebook (True)\n",
    "usefile = False # Set to \"True\" if loading from file a dicts_list to add to. Confirms with user input first!\n",
    "workstation = False # If working from office PC\n",
    "numcpus = int(6) # For multiprocessing\n",
    "\n",
    "if notebook:\n",
    "    usefile = False # Prompting user for input file is only useful in command-line\n",
    "\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\",\n",
    "               \"em\", \"kbd\", \"strong\", \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\",\n",
    "               \"span\", \"sub\", \"sup\"] # this list helps with eliminating junk tags when parsing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set directories\n",
    "\n",
    "if workstation and notebook:\n",
    "    dir_prefix = \"C:\\\\Users\\\\Jaren\\\\Documents\\\\Charter-school-identities\\\\\" # One level further down than the others\n",
    "elif notebook:\n",
    "    dir_prefix = \"/home/jovyan/work/\"\n",
    "else:\n",
    "    dir_prefix = \"/vol_b/data/\"\n",
    "\n",
    "example_page = \"https://westlakecharter.com/about/\"\n",
    "example_schoolname = \"TWENTY-FIRST_CENTURY_NM\"\n",
    "\n",
    "if workstation and notebook:\n",
    "    micro_sample13 = dir_prefix + \"data\\\\micro-sample13_coded.csv\" # Random micro-sample of 300 US charter schools\n",
    "    URL_schooldata = dir_prefix + \"data\\\\charter_URLs_2014.csv\" # 2014 population of 6,973 US charter schools\n",
    "    full_schooldata = dir_prefix + \"data\\\\charter_merged_2014.csv\" # Above merged with PVI, EdFacts, year opened/closed\n",
    "    temp_data = dir_prefix + \"data\\\\school_parser_temp.json\" # Full_schooldata dict with output for some schools\n",
    "    example_file = dir_prefix + \"data\\\\example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "    dicts_dir = dir_prefix + \"dicts\\\\\" # Directory in which to find & save dictionary files\n",
    "    save_dir = dir_prefix + \"data\\\\\" # Directory in which to save data files\n",
    "    temp_dir = dir_prefix + \"data\\\\temp\\\\\" # Directory in which to save temporary data files\n",
    "\n",
    "else:\n",
    "    wget_dataloc = dir_prefix + \"wget/parll_wget/\" #data location for schools downloaded with wget in parallel (requires server access)\n",
    "    example_folder = wget_dataloc + \"TWENTY-FIRST_CENTURY_NM/\" # Random charter school folder\n",
    "    example_file = dir_prefix + \"wget/example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "\n",
    "    micro_sample13 = dir_prefix + \"Charter-school-identities/data/micro-sample13_coded.csv\" #data location for random micro-sample of 300 US charter schools\n",
    "    URL_schooldata = dir_prefix + \"Charter-school-identities/data/charter_URLs_2014.csv\" #data location for 2014 population of US charter schools\n",
    "    full_schooldata = dir_prefix + \"Charter-school-identities/data/charter_merged_2014.csv\" # Above merged with PVI, EdFacts, year opened/closed\n",
    "    temp_data = dir_prefix + \"Charter-school-identities/data/school_parser_temp.json\" # Full_schooldata dict with output for some schools\n",
    "    dicts_dir = dir_prefix + \"Charter-school-identities/dicts/\" # Directory in which to find & save dictionary files\n",
    "    save_dir = dir_prefix + \"Charter-school-identities/data/\" # Directory in which to save data files\n",
    "    temp_dir = dir_prefix + \"Charter-school-identities/data/temp/\" # Directory in which to save temporary data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging options\n",
    "log_file = temp_dir + \"logfile_\" + str(datetime.today()) + \".log\"\n",
    "logging.basicConfig(filename=log_file,level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input file, if any\n",
    "if usefile and not notebook:\n",
    "    print(\"\\nWould you like to load from file a list of dictionaries to add to? (Y/N)\")\n",
    "    answer = input()\n",
    "    if answer == \"Y\":\n",
    "        print(\"Please indicate file path for dictionary list file.\")\n",
    "        answer2 = input()\n",
    "        if os.path.exists(answer2):\n",
    "            input_file = answer2\n",
    "            usefile = True\n",
    "        else:\n",
    "            print(\"Invalid file path\" + str(answer2) + \" \\nAborting script.\")\n",
    "            sys.exit()\n",
    "\n",
    "    elif answer == \"N\":\n",
    "        print(\"OK! This script will create a new data file at \" + str(save_dir) + \".\")\n",
    "        usefile = False\n",
    "    \n",
    "    else:\n",
    "        print(\"Error: \" + str(answer) + \" not an interpretable response. Aborting script.\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define (non-parsing) helper functions\n",
    "\n",
    "def get_vars(data):\n",
    "    \"\"\"Defines variable names based on the data source called.\"\"\"\n",
    "    \n",
    "    if data==URL_schooldata:\n",
    "        URL_variable = \"TRUE_URL\"\n",
    "        NAME_variable = \"SCH_NAME\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "        \n",
    "    elif data==full_schooldata:\n",
    "        URL_variable = \"SCH_NAME\" # Work-around until URLs merged into full data file\n",
    "        NAME_variable = \"SCH_NAME\"\n",
    "        ADDR_variable = \"ADDRESS14\"\n",
    "    \n",
    "    elif data==micro_sample13:\n",
    "        URL_variable = \"URL\"\n",
    "        NAME_variable = \"SCHNAM\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            print(\"Error processing variables from data file \" + str(data) + \"!\")\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: No data source established!\\n\")\n",
    "            print(e)\n",
    "    \n",
    "    return(URL_variable,NAME_variable,ADDR_variable)\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    \"\"\"Returns false if a web element has a non-visible tag, \n",
    "    i.e. one site visitors wouldn't actually read--and thus one we don't want to parse\"\"\"\n",
    "    \n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def webtext_from_files(datalocation):\n",
    "    \"\"\"Concatenate and return a single string from all webtext (with .txt format) in datalocation\"\"\"\n",
    "    \n",
    "    string = \"\"\n",
    "    for root, dirs, files in os.walk(datalocation):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                fileloc = open(datalocation+file, \"r\")\n",
    "                string = string + (fileloc.read())\n",
    "    return string\n",
    "\n",
    "\n",
    "def remove_spaces(file_path):\n",
    "    \"\"\"Remove spaces from text file at file_path\"\"\"\n",
    "    \n",
    "    words = [x for x in open(file_path).read().split() if x != \"\"]\n",
    "    text = \"\"\n",
    "    for word in words:\n",
    "        text += word + \" \"\n",
    "    return text\n",
    "\n",
    "\n",
    "def write_errors(error_file, error1, error2, error3, file_count):\n",
    "    \"\"\"Writes to error_file three binary error flags derived from parse_school(): \n",
    "    duplicate_flag, parse_error_flag, wget_fail_flag, and file_count.\"\"\"\n",
    "    \n",
    "    with open(error_file, 'w') as file_handler:\n",
    "        file_handler.write(\"duplicate_flag {}\\n\".format(int(error1)))\n",
    "        file_handler.write(\"parse_error_flag {}\\n\".format(int(error2)))\n",
    "        file_handler.write(\"wget_fail_flag {}\\n\".format(int(error3)))\n",
    "        file_handler.write(\"file_count {}\".format(int(file_count)))\n",
    "        return\n",
    "    \n",
    "\n",
    "def write_counts(file_path, names_list, counts_list):\n",
    "    \"\"\"Writes to file_path the input dict_count names (a list) and counts (another list).\n",
    "    Assumes these two lists have same length and are in same order--\n",
    "    e.g., names_list[0]=\"ess_count\" and counts_list[0]=ess_count.\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        for tup in zip(names_list,counts_list): # iterate over zipped list of tuples\n",
    "            if tup != list(zip(names_list,counts_list))[-1]:\n",
    "                file_handler.write(\"{} {}\\n\".format(tup[0],tup[1]))\n",
    "            else:\n",
    "                file_handler.write(\"{} {}\".format(tup[0],tup[1]))\n",
    "        return\n",
    "\n",
    "    \n",
    "def write_list(file_path, textlist):\n",
    "    \"\"\"Writes textlist to file_path. Useful for recording output of parse_school().\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        for elem in textlist:\n",
    "            file_handler.write(\"{}\\n\".format(elem))\n",
    "        return\n",
    "    \n",
    "\n",
    "def load_list(file_path):\n",
    "    \"\"\"Loads list into memory. Must be assigned to object.\"\"\"\n",
    "    \n",
    "    textlist = []\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            textlist.append(line)\n",
    "            line = file_handler.readline()\n",
    "    return textlist\n",
    "\n",
    "        \n",
    "def save_datafile(data, file, thismode):\n",
    "    \"\"\"Saves data to file using JSON, pickle, or CSV format (whichever was specified).\n",
    "    Works with Pandas DataFrames or other objects, e.g. a list of dictionaries.\n",
    "    Deletes file first to reduce risk of data duplication.\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    thismode = str(thismode)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file) # Delete file first to reduce risk of data duplication\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if thismode.upper()==\"JSON\" or thismode.upper()==\".JSON\":\n",
    "            if not file.endswith(\".json\"):\n",
    "                file += \".json\"\n",
    "            with open(file, 'w') as outfile:\n",
    "                if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                    data.to_json(outfile)\n",
    "                else:\n",
    "                    json.dump(data, outfile)\n",
    "                print(\"Data saved to \" + file + \"!\")\n",
    "\n",
    "        elif thismode.lower()==\"pickle\" or thismode.lower()==\".pickle\":\n",
    "            if not file.endswith(\".pickle\"):\n",
    "                file += \".pickle\"\n",
    "            with open(file, \"wb\") as outfile:\n",
    "                if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                    data.to_pickle(outfile)\n",
    "                else:\n",
    "                    pickle.dump(data, outfile)\n",
    "                print(\"Data saved to \" + file + \"!\")\n",
    "                \n",
    "        elif thismode.upper()==\"CSV\" or thismode.upper()==\".CSV\":\n",
    "            if not file.endswith(\".csv\"):\n",
    "                file += \".csv\"\n",
    "            with open(file, \"w\") as outfile:\n",
    "                if type(data)==\"pandas.core.frame.DataFrame\":\n",
    "                    data.to_csv(outfile,mode=\"w\",index=False) # ,header=data.columns.values\n",
    "                else:\n",
    "                    wr = csv.writer(outfile)\n",
    "                    wr.writerows(data)\n",
    "                print(\"Data saved to \" + file + \"!\")\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR! Improper arguments. Please include: data object to save (Pandas DataFrames OK), file path, and file format ('JSON', 'pickle', or 'CSV').\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Failed to save to \" + str(file) + \" into memory using \" + str(thismode) + \" format. Please check arguments (data, file, file format) and try again.\")\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def load_datafile(file):\n",
    "    \"\"\"Loads dicts_list (or whatever) from file, using either JSON or pickle format. \n",
    "    The created object should be assigned when called.\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    \n",
    "    if file.lower().endswith(\".json\"):\n",
    "        with open(file,'r') as infile:\n",
    "            var = json.load(infile)\n",
    "    \n",
    "    if file.lower().endswith(\".pickle\"):\n",
    "        with open(file,'rb') as infile:\n",
    "            var = pickle.load(infile)\n",
    "        \n",
    "    print(file + \" successfully loaded!\")\n",
    "    return var\n",
    "\n",
    "\n",
    "def load_dict(custom_dict, file_path):\n",
    "    \"\"\"Loads in a dictionary. Adds each entry from the dict at file_path to the defined set custom_dict (the input), \n",
    "    which can also be an existing dictionary. This allows the creation of combined dictionaries!\"\"\"\n",
    "\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            custom_dict.add(stemmer.stem(line.replace(\"\\n\", \"\"))) # Add line after stemming dictionary entries and eliminating newlines\n",
    "            line = file_handler.readline() # Look for anything else in that line, add that too\n",
    "    return custom_dict\n",
    "\n",
    "\n",
    "def list_files(folder_path, extension):\n",
    "    \"\"\"Outputs a list of every file in folder_path or its subdirectories that has a specified extension.\n",
    "    Prepends specified extension with '.' if it doesn't start with it already.\n",
    "    If no extension is specified, it just returns all files in folder_path.\"\"\"\n",
    "    \n",
    "    matches = []\n",
    "    if extension:\n",
    "        extension = str(extension) # Coerce to string, just in case\n",
    "    \n",
    "    if extension and not extension.startswith(\".\"):\n",
    "        extension = \".\" + extension\n",
    "    \n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        if extension:\n",
    "            for filename in fnmatch.filter(filenames, \"*\" + extension): # Use extension to filter list of files\n",
    "                matches.append(os.path.join(dirpath,filename))\n",
    "        else:\n",
    "                matches.append(os.path.join(dirpath,filename)) # If no extension, just take all files\n",
    "    return matches\n",
    "\n",
    "\n",
    "def has_html(folder_path):\n",
    "    \"\"\"Simple function that counts .html files and returns a binary:\n",
    "    'True' if a specified folder has any .html files in it, 'False' otherwise.\"\"\"\n",
    "    \n",
    "    html_list = []\n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        for file in fnmatch.filter(filenames, \"*.html\"): # Check if any HTML files in folder_path\n",
    "            html_list.append(file)\n",
    "    \n",
    "    if len(html_list)==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def set_failflag(folder_name):\n",
    "    \"\"\"The web_fail_flag indicates whether the webcrawl/download operation failed to capture any .html for a particular folder_name.\n",
    "    This function sets the web_fail_flag depending on two conditions: \n",
    "    (1) Whether or not there exists a web download folder corresponding to folder_name, and\n",
    "    (2) Whether or not that folder contains at least one file with the .html extension.\"\"\"\n",
    "    \n",
    "    global wget_dataloc #,dicts_list # Need access to the list of dictionaries\n",
    "    web_fail_flag = \"\" # make output a str to work with currently limited Pandas dtype conversion functionality\n",
    "    \n",
    "    folder_path = str(wget_dataloc) + folder_name + \"/\"\n",
    "    if (not os.path.exists(folder_path)) or (has_html(folder_path)==False):\n",
    "        web_fail_flag = str(1) # If folder doesn't exist, mark as fail and ignore when loading files\n",
    "    else:\n",
    "        web_fail_flag = str(0) # make str so can work with currently limited Pandas dtype conversion functionality\n",
    "    \n",
    "    #match_index = next((index for (index, d) in enumerate(dicts_list) if d[\"folder_name\"] == folder_name), None) # Find dict index of input/folder_name\n",
    "    #dicts_list[match_index]['wget_fail_flag'] = web_fail_flag # Assign output to dict entry for folder_name\n",
    "    \n",
    "    return web_fail_flag\n",
    "\n",
    "\n",
    "def convert_df(df):\n",
    "    \"\"\"Makes a Pandas DataFrame more memory-efficient through intelligent use of Pandas data types: \n",
    "    specifically, by storing columns with repetitive Python strings not with the object dtype for unique values \n",
    "    (entirely stored in memory) but as categoricals, which are represented by repeated integer values. This is a \n",
    "    net gain in memory when the reduced memory size of the category type outweighs the added memory cost of storing \n",
    "    one more thing. As such, this function checks the degree of redundancy for a given column before converting it.\n",
    "    \n",
    "    # TO DO: Filter out non-object columns, make that more efficient by downcasting numeric types using pd.to_numeric(), \n",
    "    merge  that with the converted object columns (see https://www.dataquest.io/blog/pandas-big-data/). \n",
    "    For now, since the current DF is ENTIRELY composed of object types, code is left as is. \n",
    "    But note that the current code will eliminate any non-object type columns.\"\"\"\n",
    "    \n",
    "    converted_df = pd.DataFrame() # Initialize DF for memory-efficient storage of strings (object types)\n",
    "    df_obj = df.select_dtypes(include=['object']).copy() # Filter to only those columns of object data type\n",
    "\n",
    "    for col in df.columns: \n",
    "        if col in df_obj: \n",
    "            num_unique_values = len(df_obj[col].unique())\n",
    "            num_total_values = len(df_obj[col])\n",
    "            if (num_unique_values / num_total_values) < 0.5: # Only convert data types if at least half of values are duplicates\n",
    "                converted_df.loc[:,col] = df[col].astype('category') # Store these columns as dtype \"category\"\n",
    "            else: \n",
    "                converted_df.loc[:,col] = df[col]\n",
    "        else:    \n",
    "            converted_df.loc[:,col] = df[col]\n",
    "                      \n",
    "    converted_df.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='float')\n",
    "    converted_df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='signed')\n",
    "    \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set parsing keywords\n",
    "\n",
    "keywords = ['values', 'academics', 'skills', 'purpose',\n",
    "                       'direction', 'mission', 'vision', 'vision', 'mission', 'our purpose',\n",
    "                       'our ideals', 'ideals', 'our cause', 'curriculum','curricular',\n",
    "                       'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system',\n",
    "                       'structure','philosophy', 'philosophical', 'beliefs', 'believe',\n",
    "                       'principles', 'creed', 'credo', 'values','moral', 'history', 'our story',\n",
    "                       'the story', 'school story', 'background', 'founding', 'founded',\n",
    "                       'established','establishment', 'our school began', 'we began',\n",
    "                       'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "                       'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = ['mission','vision', 'vision:', 'mission:', 'our purpose', 'our ideals', 'ideals:', 'our cause', 'cause:', 'goals', 'objective']\n",
    "curriculum_keywords = ['curriculum', 'curricular', 'program', 'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system', 'structure']\n",
    "philosophy_keywords = ['philosophy', 'philosophical', 'beliefs', 'believe', 'principles', 'creed', 'credo', 'value',  'moral']\n",
    "history_keywords = ['history', 'story','our story', 'the story', 'school story', 'background', 'founding', 'founded', 'established', 'establishment', 'our school began', 'we began', 'doors opened', 'school opened']\n",
    "about_keywords =  ['about us', 'our school', 'who we are', 'overview', 'general information', 'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = set(stemmer.stem(word) for word in mission_keywords)\n",
    "curriculum_keywords = set(stemmer.stem(word) for word in curriculum_keywords)\n",
    "philosophy_keywords = set(stemmer.stem(word) for word in philosophy_keywords)\n",
    "history_keywords = set(stemmer.stem(word) for word in history_keywords)\n",
    "about_keywords =  set(stemmer.stem(word) for word in about_keywords)\n",
    "all_keywords = set(stemmer.stem(key) for key in keywords)\n",
    "\n",
    "logging.info(\"List of keywords:\\n\" + str(list(all_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create dictionaries for each ideology and one for combined ideologies\n",
    "\n",
    "ess_dict, prog_dict, rit_dict, all_ideol, all_dicts = set(), set(), set(), set(), set()\n",
    "all_ideol = load_dict(all_ideol, dicts_dir + \"ess_dict.txt\")\n",
    "all_ideol = load_dict(all_ideol, dicts_dir + \"prog_dict.txt\") # For complete ideological list, append second ideological dict\n",
    "all_dicts = load_dict(all_ideol, dicts_dir + \"rit_dict.txt\") # For complete dict list, append ritual dict terms too\n",
    "ess_dict = load_dict(ess_dict, dicts_dir + \"ess_dict.txt\")\n",
    "prog_dict = load_dict(prog_dict, dicts_dir + \"prog_dict.txt\")\n",
    "rit_dict = load_dict(rit_dict, dicts_dir + \"rit_dict.txt\")\n",
    "\n",
    "logging.info(str(len(all_ideol)) + \" entries loaded into the combined ideology dictionary.\")\n",
    "list_dict = list(all_ideol)\n",
    "list_dict.sort(key = lambda x: x.lower())\n",
    "logging.info(\"First 10 elements of combined ideology dictionary are:\\n\" + str(list_dict[:10]))\n",
    "    \n",
    "\n",
    "# Create tuples for keyword lists and dictionary terms:\n",
    "keys_tuple = tuple([mission_keywords,curriculum_keywords,philosophy_keywords,history_keywords,about_keywords,\\\n",
    "                        all_ideol,all_keywords])\n",
    "dicts_tuple = tuple([ess_dict,prog_dict,rit_dict,all_dicts])\n",
    "    \n",
    "logging.info(str(list(keys_tuple)))\n",
    "logging.info(str(list(dicts_tuple)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define dictionary matching helper functions\n",
    "\n",
    "def dict_count(text_list, custom_dict):\n",
    "    \n",
    "    \"\"\"Performs dictionary analysis, returning number of dictionary hits found.\n",
    "    Removes punctuation and stems the phrase being analyzed. \n",
    "    Compatible with multiple-word dictionary elements.\"\"\"\n",
    "    \n",
    "    counts = 0 # number of matches between text_list and custom_dict\n",
    "    dictless_list = [] # Updated text_list with dictionary hits removed\n",
    "    max_entry_length = max([len(entry.split()) for entry in custom_dict]) # Get length (in words) of longest entry in combined dictionary\n",
    "    \n",
    "    for chunk in text_list: # chunk may be several sentences or possibly paragraphs long\n",
    "        chunk = re.sub(r'[^\\w\\s]', '', chunk) # Remove punctuation with regex that keeps only letters and spaces\n",
    "\n",
    "        # Do dictionary analysis for word chunks of lengths max_entry_length down to 1, removing matches each time.\n",
    "        # This means longer dict entries will get removed first, useful in case they contain smaller entries.\n",
    "        for length in range(max_entry_length, 0, -1):\n",
    "            dictless_chunk,len_counts = dict_match_len(chunk,custom_dict,length)\n",
    "            dictless_list.append(dictless_chunk)\n",
    "            counts += len_counts\n",
    "    \n",
    "    return dictless_list,int(counts)\n",
    "\n",
    "def dict_match_len(phrase, custom_dict, length):\n",
    "    \n",
    "    \"\"\"Helper function to dict_match. \n",
    "    Returns # dictionary hits and updated copy of phrase with dictionary hits removed. \n",
    "    Stems phrases before checking for matches.\"\"\"\n",
    "    \n",
    "    hits_indices, counts = [], 0\n",
    "    splitted_phrase = phrase.split()\n",
    "    if len(splitted_phrase) < length:\n",
    "        return phrase, 0 # If text chunk is shorter than length of dict entries being matched, don't continue.\n",
    "    \n",
    "    for i in range(len(splitted_phrase) - length + 1):\n",
    "        to_stem = \"\"\n",
    "        for j in range(length):\n",
    "            to_stem += splitted_phrase[i+j] + \" \" # Builds chunk of 'length' words\n",
    "        stemmed_word = stemmer.stem(to_stem[:-1]) # stem chunk\n",
    "        if stemmed_word in custom_dict:\n",
    "            hits_indices.append(i) # Store the index of the word that has a dictionary hit\n",
    "            counts += 1\n",
    "            logging.info(stemmed_word)\n",
    "                \n",
    "    # Iterate through list of matching word indices and remove the matches\n",
    "    for i in range(len(hits_indices)-1, -1, -1):\n",
    "        splitted_phrase = splitted_phrase[:hits_indices[i]] + \\\n",
    "        splitted_phrase[hits_indices[i] + length:]\n",
    "    modified_phrase = \"\"\n",
    "    for sp in splitted_phrase: # Rebuild the modified phrase, with matches removed\n",
    "        modified_phrase += sp + \" \"\n",
    "    return modified_phrase[:-1], counts\n",
    "\n",
    "                  \n",
    "@timeout_decorator.timeout(20, use_signals=False)\n",
    "def dictmatch_file_helper(file,dictsnames_biglist,all_keywords,all_ideol,all_matches):\n",
    "    \"\"\"Counts number of matches in file for each list of terms given, and also collects the terms not matched.\n",
    "    Dictsnames_biglist is a list of lists, each list containing:\n",
    "    a list of key terms, currently essentialism, progressivism, ritualism, and all three combined (ess_dict, prog_dict, rit_dict, all_dicts);\n",
    "    the variables used to store the number of matches for each term lit (ess_count, prog_count, rit_count, alldict_count);\n",
    "    and the not-matches--that is, the list of words leftover from the file after all matches are removed (ess_dictless, prog_dictless, rit_dictless, alldict_dictless). \"\"\"\n",
    "    \n",
    "    \"\"\"OLD METHOD:\n",
    "    for adict,count_name,dictless_name in dictsnames_tupzip: # Iterate over dicts to find matches with parsed text of file\n",
    "        count_add = 0 # Initialize iterator for dict-specific count matches\n",
    "        dictless_add,count_add = dict_count(parsed_pagetext,adict)\n",
    "        count_name += count_add\n",
    "        dictless_name += dictless_add\n",
    "        all_matches += count_add\n",
    "                    \n",
    "        # Now use this count to update zipped list of tuples, so each iteration adds to the total dict_count:\n",
    "        dictsnames_list = [ess_count, prog_count, rit_count, alldict_count] \n",
    "        dictsnames_tupzip = zip(dicts_tuple, dictsnames_list, dictlessnames_list) \n",
    "                    \n",
    "        logging.info(\"  Discovered \" + str(count_add) + \" matches for \" + str(file) + \", a total thus far of \" + str(count_name) + \" matches...\")\"\"\"\n",
    "                  \n",
    "    for i in range(len(dictsnames_biglist)): # Iterate over dicts to find matches with parsed text of file\n",
    "        # Dicts are: (ess_dict, prog_dict, rit_dict, alldict_count); count_names are: (ess_count, prog_count, rit_count, alldict_count); dictless_names are: (ess_dictless, prog_dictless, rit_dictless, alldict_dictless)\n",
    "        # adict,count_name,dictless_name = dictsnames_tupzip[i]\n",
    "        dictless_add,count_add = dict_count(parsed_pagetext,dictsnames_biglist[i][0])\n",
    "        dictsnames_biglist[i][1] += count_add\n",
    "        dictsnames_biglist[i][2] += dictless_add\n",
    "        all_matches += count_add\n",
    "                    \n",
    "        logging.info(\"Discovered \" + str(count_add) + \" matches for \" + str(file) + \", a total thus far of \" + str(dictsnames_biglist[i][1]) + \" matches...\")\n",
    "                  \n",
    "    return dictsnames_biglist,all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define parsing helper functions\n",
    "\n",
    "def parsefile_by_tags(HTML_file):\n",
    "    \n",
    "    \"\"\"Cleans HTML by removing inline tags, ripping out non-visible tags, \n",
    "    replacing paragraph tags with a random string, and finally using this to separate HTML into chunks.\n",
    "    Reads in HTML from storage using a given filename, HTML_file.\"\"\"\n",
    "\n",
    "    random_string = \"\".join(map(chr, os.urandom(75))) # Create random string for tag delimiter\n",
    "    soup = BeautifulSoup(open(HTML_file), bsparser)\n",
    "    \n",
    "    [s.extract() for s in soup(['style', 'script', 'head', 'title', 'meta', '[document]'])] # Remove non-visible tags\n",
    "    for it in inline_tags:\n",
    "        [s.extract() for s in soup(\"</\" + it + \">\")] # Remove inline tags\n",
    "    \n",
    "    visible_text = soup.getText(random_string).replace(\"\\n\", \"\") # Replace \"p\" tags with random string, eliminate newlines\n",
    "    # Split text into list using random string while also eliminating tabs and converting unicode to readable text:\n",
    "    visible_text = list(normalize(\"NFKC\",elem.replace(\"\\t\",\"\")) for elem in visible_text.split(random_string))\n",
    "    visible_text = list(filter(lambda vt: vt.split() != [], visible_text)) # Eliminate empty elements\n",
    "    # Consider joining list elements together with newline in between by prepending with: \"\\n\".join\n",
    "\n",
    "    return(visible_text)\n",
    "\n",
    "\n",
    "example_textlist = parsefile_by_tags(example_file)\n",
    "logging.info(\"Output of parsefile_by_tags:\\n\" + str(example_textlist))\n",
    "\n",
    "    \n",
    "@timeout_decorator.timeout(20, use_signals=False)\n",
    "def parse_file_helper(file,webtext,keywords_text,ideology_text):\n",
    "    \"\"\"Parses file into (visible) webtext, both complete and filtered by terms in 'keywords' and 'ideology' lists.\"\"\"\n",
    "    \n",
    "    parsed_pagetext = []\n",
    "    parsed_pagetext = parsefile_by_tags(file) # Parse page text\n",
    "\n",
    "    if len(parsed_pagetext) == 0: # Don't waste time adding empty pages\n",
    "        logging.warning(\"    Nothing to parse in \" + str(file) + \"!\")\n",
    "    \n",
    "    else:\n",
    "        webtext.extend(parsed_pagetext) # Add new parsed text to long list\n",
    "        keywords_text.extend(filter_dict_page(parsed_pagetext, all_keywords)) # Filter using keywords\n",
    "        ideology_text.extend(filter_dict_page(parsed_pagetext, all_ideol)) # Filter using ideology words\n",
    "\n",
    "        logging.info(\"Successfully parsed and filtered file \" + str(file) + \"...\")\n",
    "        \n",
    "    return webtext,keywords_text,ideology_text\n",
    "                  \n",
    "\n",
    "def filter_dict_page(pagetext_list, keyslist):\n",
    "    \n",
    "    \"\"\"Filters webtext of a given .html page, which is parsed and in list format, to only those strings \n",
    "    within pagetext_list containing an element (word or words) of inputted keyslist. \n",
    "    Returns list filteredtext wherein each element has original case (not coerced to lower-case).\"\"\"\n",
    "    \n",
    "    filteredtext = [] # Initialize empty list to hold strings of page\n",
    "    \n",
    "    for string in pagetext_list:\n",
    "        lowercasestring = str(string).lower() # lower-case string...\n",
    "        dict_list = [key.lower() for key in list(keyslist)] # ...compared with lower-case element of keyslist\n",
    "        for key in dict_list:\n",
    "            if key in lowercasestring and key in lowercasestring.split(' '): # Check that the word is the whole word not part of another one\n",
    "                filteredtext.append(string)\n",
    "\n",
    "    return filteredtext\n",
    "\n",
    "\n",
    "logging.info(\"Output of filter_keywords_page with keywords:\\n\" + str(filter_dict_page(example_textlist, all_keywords)))   \n",
    "logging.info(\"Output of filter_keywords_page with ideology words:\\n\\n\" + str(filter_dict_page(example_textlist, all_ideol)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_school(schooltup):\n",
    "    \n",
    "    \"\"\"This core function parses webtext for a given school. Input is tuple: (name, address, url).\n",
    "    It uses helper functions to run analyses and then returning multiple outputs:\n",
    "    full (partially cleaned) webtext, by parsing webtext of each .html file (removing inline tags, etc.) within school's folder, via parsefile_by_tags();\n",
    "    all text associated with specific categories by filtering webtext to those with elements from a defined keyword list, via filter_keywords_page();\n",
    "    AND COUNTS FOR DICT MATCHES\n",
    "    \n",
    "    For the sake of parsimony and manageable script calls, OTHER similar functions/scripts return these additional outputs: \n",
    "    parsed webtext, having removed overlapping headers/footers common to multiple pages, via remove_overlaps();\n",
    "    all text associated with specific categories by filtering webtext according to keywords for \n",
    "    mission, curriculum, philosophy, history, and about/general self-description, via categorize_page(); and\n",
    "    contents of those individual pages best matching each of these categories, via find_best_categories.\"\"\"\n",
    "    \n",
    "    global itervar,numschools,parsed,wget_dataloc,dicts_list,keys_tuple,dicts_tuple # Access variables defined outside function (globally)\n",
    "        \n",
    "    itervar +=1 # Count school\n",
    "    datalocation = wget_dataloc # Define path to local data storage\n",
    "    school_name,school_address,school_URL,folder_name = schooltup[0],schooltup[1],schooltup[2],schooltup[3] # Assign variables from input tuple (safe because element order for a tuple is immutable)\n",
    "    \n",
    "    logging.info(\"Parsing \" + str(school_name) + \" at \" + str(school_address) + \" in folder <\" + datalocation + str(folder_name) + \"/>, which is ROUGHLY #\" + str(6*itervar) + \" / \" + str(numschools) + \" schools...\")\n",
    "    \n",
    "    school_folder = datalocation + folder_name + \"/\"\n",
    "    error_file = school_folder + \"error_flags.txt\" # Define file path for error text log\n",
    "    counts_file = school_folder + \"dict_counts.txt\" # File path for dictionary counts output\n",
    "    \n",
    "    if school_URL==school_name:\n",
    "        school_URL = folder_name # Workaround for full_schooldata, which doesn't yet have URLs\n",
    "    \n",
    "    # PRELIMINARY TEST 1: Check if parsing is already done. If so, no need to parse--stop function!\n",
    "    if os.path.exists(error_file) and os.path.exists(counts_file):\n",
    "        logging.info(\"Parsing output already detected in \" + str(school_folder) + \", aborting parser...\")\n",
    "        return\n",
    "    \n",
    "    # PRELIMINARY TEST 2: Check if folder exists. If not, nothing to parse. Thus, do not pass go; do not continue function.\n",
    "    duplicate_flag,parse_error_flag,wget_fail_flag,file_count = 0,0,0,0 # initialize error flags\n",
    "    \n",
    "    if not (os.path.exists(school_folder) or os.path.exists(school_folder.lower()) or os.path.exists(school_folder.upper())):\n",
    "        logging.warning(\"NO DIRECTORY FOUND, creating \" + str(school_folder) + \" for 'error_flags.txt' and aborting...\")\n",
    "        wget_fail_flag = 1\n",
    "        try:\n",
    "            os.makedirs(school_folder) # Create empty folder for school to hold error_flags.txt (and nothing else)\n",
    "            write_errors(error_file, duplicate_flag, parse_error_flag, wget_fail_flag, file_count)\n",
    "            write_counts(counts_file, [\"ess_count\",\"prog_count\",\"rit_count\"], [0,0,0]) # empty counts file simplifies parsing\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logging.debug(\"Uh-oh! Failed to log error flags for \" + str(school_name) + \".\\n\" + e)\n",
    "            return\n",
    "    \n",
    "    # PRELIMINARY TEST 3: Check if this school has already been parsed--via its unique school_URL. If so, skip this school to avoid duplication bias.\n",
    "    if school_URL in parsed: \n",
    "        logging.error(\"DUPLICATE URL DETECTED. Skipping \" + str(school_folder) + \"...\")\n",
    "        duplicate_flag = 1\n",
    "        write_errors(error_file, duplicate_flag, parse_error_flag, wget_fail_flag, file_count)\n",
    "        write_counts(counts_file, [\"ess_count\",\"prog_count\",\"rit_count\"], [0,0,0]) # empty counts file simplifies parsing\n",
    "        return\n",
    "    \n",
    "    logging.info(\"Preliminary tests passed. Parsing data in \" + str(school_folder) + \"...\")\n",
    "    \n",
    "    # Next, initialize local (within-function) variables for text output\n",
    "    webtext,keywords_text,ideology_text,dictless_words = [],[],[],[] # text category lists\n",
    "    file_list = [] # list of HTML files in school_folder\n",
    "    \n",
    "    mission,curriculum,philosophy,history,about,ideology,keywords = [],[],[],[],[],[],[] # matched keyword lists\n",
    "    ess_count, prog_count, rit_count, alldict_count, all_matches = 0,0,0,0,0 # dict match counts\n",
    "    ess_dictless, prog_dictless, rit_dictless, alldict_dictless = [],[],[],[] # lists of unmatched words. Why?\n",
    "    # Later we can revise the dictionaries by looking at what content words were not counted by current dictionaries. \n",
    "\n",
    "    titles_list = [mission,curriculum,philosophy,history,about,ideology,keywords] # list of matched keyword lists\n",
    "    dictsnames_list = [ess_count, prog_count, rit_count, alldict_count] # list of dict match counts\n",
    "    dictlessnames_list = [ess_dictless, prog_dictless, rit_dictless, alldict_dictless] # list of unmatched word lists\n",
    "\n",
    "    # Now link together dict terms lists with variables holding their matches and their not-matches:\n",
    "    keysnames_tupzip = zip(keys_tuple, titles_list) # zips together keyword lists with the variables holding their matches\n",
    "    #dictsnames_tuplist = zip(dicts_tuple, dictsnames_list, dictlessnames_list)\n",
    "    dictsnames_biglist = [[dicts_tuple[i],dictsnames_list[i],dictlessnames_list[i]] for i in range(len(dicts_tuple))]\n",
    "\n",
    "    logging.info(str(list(keysnames_tupzip)))\n",
    "    logging.info(str(list(dictsnames_tupzip)))\n",
    "    \n",
    "    # Now to parsing:\n",
    "    try:\n",
    "        # Parse file only if it contains HTML. This is easy: use the \"*.html\" wildcard pattern--\n",
    "        # also wget gave the \".html\" file extension to appropriate files when downloading (`--adjust-extension` option)\n",
    "        # Less efficient ways to check if files contain HTML (e.g., for data not downloaded by wget):\n",
    "        # if bool(BeautifulSoup(open(fname), bsparser).find())==True: # if file.endswith(\".html\"):\n",
    "        # Another way to do this, maybe faster but broken: files_iter = iglob(school_folder + \"**/*.html\", recursive=True)\n",
    "            \n",
    "        file_list = list_files(school_folder, \".html\") # Get list of HTML files in school_folder\n",
    "            \n",
    "        if file_list==(None or school_folder or \"\" or []) or not file_list or len(file_list)==0:\n",
    "            logging.info(\"No .html files found. Aborting parser for \" + str(school_name) + \"...\")\n",
    "            wget_fail_flag = 1\n",
    "            write_errors(error_file, duplicate_flag, parse_error_flag, wget_fail_flag, file_count)\n",
    "            write_counts(counts_file, [\"ess_count\",\"prog_count\",\"rit_count\"], [0,0,0]) # empty counts file simplifies parsing\n",
    "            return\n",
    "        \n",
    "        for file in tqdm(file_list, desc=(\"Parsing files\")):\n",
    "                \n",
    "            logging.info(\"Parsing HTML in \" + str(file) + \"...\")\n",
    "                    \n",
    "            # Parse and categorize page text:\n",
    "            try:                    \n",
    "                webtext,keywords_text,ideology_text = parse_file_helper(file, webtext, keywords_text, ideology_text)\n",
    "                        \n",
    "                file_count+=1 # add to count of parsed files\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(\"ERROR! Failed to parse file...\\n\" + e)\n",
    "                        \n",
    "            # Count dict matches:\n",
    "            try:\n",
    "                dictsnames_biglist,all_matches = dictmatch_file_helper(file,dictsnames_biglist, all_keywords, all_ideol, all_matches)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.info(\"ERROR! Failed to count number of dict matches while parsing \" + str(file) + \"...\\n\" + e)\n",
    "                    \n",
    "        # Report and save output to disk:\n",
    "        print(\"Got here 1\")\n",
    "        parsed.append(school_URL)\n",
    "        file_count = int(file_count-1)\n",
    "        print(\"  PARSED \" + str(file_count) + \" .html file(s) from website of \" + str(school_name) + \"...\")\n",
    "            \n",
    "        write_list(school_folder + \"webtext.txt\", webtext)\n",
    "        write_list(school_folder + \"keywords_text.txt\", keywords_text)\n",
    "        write_list(school_folder + \"ideology_text.txt\", ideology_text)\n",
    "            \n",
    "        print(\"  Found \" + str(all_matches) + \" total dictionary matches and \" + str(len(dictsnames_biglist[3][2])) + \" uncounted words for \" + str(school_name) + \"...\")\n",
    "\n",
    "        write_counts(counts_file, [\"ess_count\",\"prog_count\",\"rit_count\"], [dictsnames_biglist[0][1], dictsnames_biglist[1][1], dictsnames_biglist[2][1]])\n",
    "        print(\"Got here 2\")\n",
    "        write_list(school_folder + \"dictless_words.txt\", dictsnames_biglist[3][2])\n",
    "        print(\"Got here 3\")\n",
    "                    \n",
    "        write_errors(error_file, duplicate_flag, parse_error_flag, wget_fail_flag, file_count)\n",
    "        print(\"Got here 4\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"ERROR! Failed to parse, categorize, and get dict matches on webtext of \" + str(school_name) + \"...\\n\" + e)\n",
    "        parse_error_flag = 1\n",
    "        write_errors(error_file, duplicate_flag, parse_error_flag, wget_fail_flag, file_count)\n",
    "        write_counts(counts_file, [\"ess_count\",\"prog_count\",\"rit_count\"], [0,0,0]) # empty counts file simplifies parsing\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def dictify_webtext(school_dict):\\n    \"\"\"Reads parsing output from text files and saves to school_dict multiple parsing outputs:\\n    webtext, keywords_text, ideology_text, file_count, etc.\"\"\"\\n    \\n    # Allow function to access these variables already defined outside the function (globally)\\n    global itervar,numschools,parsed,wget_dataloc,URL_var,NAME_var,ADDR_var,save_dir\\n    \\n    datalocation = wget_dataloc # Define path to local data storage\\n    school_name, school_address, school_URL = school_dict[NAME_var], school_dict[ADDR_var], school_dict[URL_var] # Define varnames\\n    itervar+=1 # Count this school\\n    \\n    print(\"Loading into dict parsing output for \" + str(school_name) + \", which is school #\" + str(itervar) + \" of \" + str(numschools) + \"...\")\\n    \\n    school_dict[\"webtext\"], school_dict[\"keywords_text\"], school_dict[\"ideology_text\"] = [[] for _ in range(3)]\\n    school_dict[\"duplicate_flag\"], school_dict[\"parse_error_flag\"], school_dict[\"wget_fail_flag\"] = [0 for _ in range(3)]\\n    school_dict[\\'ess_strength\\'],school_dict[\\'prog_strength\\'] = [0.0 for _ in range(2)]\\n    \\n    folder_name = school_dict[\"folder_name\"]\\n    \\n    school_folder = datalocation + folder_name + \"/\"\\n    error_file = school_folder + \"error_flags.txt\" # Define file path for error text log\\n    \\n    if school_URL==school_name:\\n        school_URL = folder_name # Workaround for full_schooldata, which doesn\\'t yet have URLs\\n\\n    # Check if folder exists. If not, exit function\\n    if not (os.path.exists(school_folder) or os.path.exists(school_folder.lower()) or os.path.exists(school_folder.upper())):\\n        print(\"  !! NO DIRECTORY FOUND matching \" + str(school_folder) + \". Aborting dictify function...\")\\n        school_dict[\\'wget_fail_flag\\'] = 1\\n        return\\n    \\n    try:\\n        # Load school parse output from disk into dictionary \\n        school_dict[\"webtext\"] = load_list(school_folder + \"webtext.txt\")\\n        school_dict[\"keywords_text\"] = load_list(school_folder + \"keywords_text.txt\")\\n        school_dict[\"ideology_text\"] = load_list(school_folder + \"ideology_text.txt\")                        \\n        \\n        \"\"\" # Comment out until dict_count is run\\n        school_dict[\"ess_count\"] = load_list(school_folder + \"ess_count.txt\")\\n        school_dict[\"prog_count\"] = load_list(school_folder + \"prog_count.txt\")\\n        school_dict[\"rit_count\"] = load_list(school_folder + \"rit_count.txt\")\\n        school_dict[\\'ess_strength\\'] = float(school_dict[\\'ess_count\\'])/float(school_dict[\\'rit_count\\'])\\n        school_dict[\\'prog_strength\\'] = float(school_dict[\\'prog_count\\'])/float(school_dict[\\'rit_count\\'])\\n        \"\"\"\\n\\n        # load error_file as a list with four pieces, the last element of each of which is the flag value itself:\\n        error_text = load_list(error_file) \\n        school_dict[\"duplicate_flag\"] = error_text[0].split()[-1] # last element of first piece of error_text\\n        school_dict[\"parse_error_flag\"] = error_text[1].split()[-1]\\n        school_dict[\"wget_fail_flag\"] = error_text[2].split()[-1]\\n        school_dict[\"html_file_count\"] = error_text[3].split()[-1]\\n        \\n        if int(school_dict[\"html_file_count\"])==0:\\n            school_dict[\"wget_fail_flag\"] = 1 # If no HTML, then web download failed!\\n        \\n        print(\"  LOADED \" + school_dict[\"html_file_count\"] + \" .html file(s) from website of \" + str(school_name) + \"...\")\\n        #save_datafile(dicts_list, save_dir+\"school_parser_temp\", \"JSON\") # Save output so we can pick up where left off, in case something breaks before able to save final output\\n        return school_dict\\n    \\n    except Exception as e:\\n        print(\"  ERROR! Failed to load into dict parsing output for \" + str(school_name))\\n        print(\"  \",e)\\n        school_dict[\"parse_error_flag\"] = 1\\n        return'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def dictify_webtext(school_dict):\n",
    "    \"\"\"Reads parsing output from text files and saves to school_dict multiple parsing outputs:\n",
    "    webtext, keywords_text, ideology_text, file_count, etc.\"\"\"\n",
    "    \n",
    "    # Allow function to access these variables already defined outside the function (globally)\n",
    "    global itervar,numschools,parsed,wget_dataloc,URL_var,NAME_var,ADDR_var,save_dir\n",
    "    \n",
    "    datalocation = wget_dataloc # Define path to local data storage\n",
    "    school_name, school_address, school_URL = school_dict[NAME_var], school_dict[ADDR_var], school_dict[URL_var] # Define varnames\n",
    "    itervar+=1 # Count this school\n",
    "    \n",
    "    print(\"Loading into dict parsing output for \" + str(school_name) + \", which is school #\" + str(itervar) + \" of \" + str(numschools) + \"...\")\n",
    "    \n",
    "    school_dict[\"webtext\"], school_dict[\"keywords_text\"], school_dict[\"ideology_text\"] = [[] for _ in range(3)]\n",
    "    school_dict[\"duplicate_flag\"], school_dict[\"parse_error_flag\"], school_dict[\"wget_fail_flag\"] = [0 for _ in range(3)]\n",
    "    school_dict['ess_strength'],school_dict['prog_strength'] = [0.0 for _ in range(2)]\n",
    "    \n",
    "    folder_name = school_dict[\"folder_name\"]\n",
    "    \n",
    "    school_folder = datalocation + folder_name + \"/\"\n",
    "    error_file = school_folder + \"error_flags.txt\" # Define file path for error text log\n",
    "    \n",
    "    if school_URL==school_name:\n",
    "        school_URL = folder_name # Workaround for full_schooldata, which doesn't yet have URLs\n",
    "\n",
    "    # Check if folder exists. If not, exit function\n",
    "    if not (os.path.exists(school_folder) or os.path.exists(school_folder.lower()) or os.path.exists(school_folder.upper())):\n",
    "        print(\"  !! NO DIRECTORY FOUND matching \" + str(school_folder) + \". Aborting dictify function...\")\n",
    "        school_dict['wget_fail_flag'] = 1\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load school parse output from disk into dictionary \n",
    "        school_dict[\"webtext\"] = load_list(school_folder + \"webtext.txt\")\n",
    "        school_dict[\"keywords_text\"] = load_list(school_folder + \"keywords_text.txt\")\n",
    "        school_dict[\"ideology_text\"] = load_list(school_folder + \"ideology_text.txt\")                        \n",
    "        \n",
    "        \"\"\" # Comment out until dict_count is run\n",
    "        school_dict[\"ess_count\"] = load_list(school_folder + \"ess_count.txt\")\n",
    "        school_dict[\"prog_count\"] = load_list(school_folder + \"prog_count.txt\")\n",
    "        school_dict[\"rit_count\"] = load_list(school_folder + \"rit_count.txt\")\n",
    "        school_dict['ess_strength'] = float(school_dict['ess_count'])/float(school_dict['rit_count'])\n",
    "        school_dict['prog_strength'] = float(school_dict['prog_count'])/float(school_dict['rit_count'])\n",
    "        \"\"\"\n",
    "\n",
    "        # load error_file as a list with four pieces, the last element of each of which is the flag value itself:\n",
    "        error_text = load_list(error_file) \n",
    "        school_dict[\"duplicate_flag\"] = error_text[0].split()[-1] # last element of first piece of error_text\n",
    "        school_dict[\"parse_error_flag\"] = error_text[1].split()[-1]\n",
    "        school_dict[\"wget_fail_flag\"] = error_text[2].split()[-1]\n",
    "        school_dict[\"html_file_count\"] = error_text[3].split()[-1]\n",
    "        \n",
    "        if int(school_dict[\"html_file_count\"])==0:\n",
    "            school_dict[\"wget_fail_flag\"] = 1 # If no HTML, then web download failed!\n",
    "        \n",
    "        print(\"  LOADED \" + school_dict[\"html_file_count\"] + \" .html file(s) from website of \" + str(school_name) + \"...\")\n",
    "        #save_datafile(dicts_list, save_dir+\"school_parser_temp\", \"JSON\") # Save output so we can pick up where left off, in case something breaks before able to save final output\n",
    "        return school_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"  ERROR! Failed to load into dict parsing output for \" + str(school_name))\n",
    "        print(\"  \",e)\n",
    "        school_dict[\"parse_error_flag\"] = 1\n",
    "        return'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandify_webtext(df):\n",
    "    \"\"\"Reads parsing output from text files and saves to DataFrame df multiple parsing outputs:\n",
    "    webtext, keywords_text, ideology_text, file_count, dict_count outputs, etc.\"\"\"\n",
    "    \n",
    "    # Allow function to access these variables already defined outside the function (globally)\n",
    "    global numschools,parsed,wget_dataloc,URL_var,NAME_var,ADDR_var,save_dir\n",
    "    \n",
    "    datalocation = wget_dataloc # Define path to local data storage\n",
    "    # VARNAMES ARE: school_name, school_address, school_URL = df[NAME_var], df[ADDR_var], df[URL_var]\n",
    "    \n",
    "    #print(\"Loading into DataFrame parsing output for \" + str(len(df)) + \" school websites out of a total of \" + str(numschools) + \"...\")\n",
    "    # df[\"folder_name\"] = df[[NAME_var, ADDR_var]].apply(lambda x: re.sub(\" \",\"_\",(\"{} {}\".format(str(x[0], x[1][-8:-6])))), axis=1) # This gives name and state separated by \"_\"  ## school[\"folder_name\"] = re.sub(\" \",\"_\",(school[NAME_var]+\" \"+school[ADDR_var][-8:-6]))  ### Comment out while fixing parser\n",
    "    df.loc[:,\"school_folder\"] = df.loc[:,\"folder_name\"].apply(lambda x: str(datalocation) + '{}/'.format(str(x)))\n",
    "    df.loc[:,\"error_file\"] = df.loc[:,\"school_folder\"].apply(lambda x: '{}error_flags.txt'.format(str(x))) # Define file path for error text log\n",
    "    df.loc[:,\"counts_file\"] = df.loc[:,\"school_folder\"].apply(lambda x: '{}dict_counts.txt'.format(str(x)))\n",
    "    \n",
    "    # Initialize text strings and counts as empty, then convert data types:\n",
    "    empty = [\"\" for elem in range(len(df[\"NCESSCH\"]))] # Create empty string column length of longest variable (NCESCCH used for matching)\n",
    "    df = df.assign(webtext=empty, keywords_text=empty, ideology_text=empty, ess_count=empty, prog_count=empty, rit_count=empty) # Add empty columns to df\n",
    "    df.loc[:,[\"webtext\", \"keywords_text\", \"ideology_text\"]] = df.loc[:,[\"webtext\", \"keywords_text\", \"ideology_text\"]].apply(lambda x: x.astype(object)) # Convert to object type--holds text\n",
    "    df.loc[:,[\"ess_count\", \"prog_count\", \"rit_count\"]] = df.loc[:,[\"ess_count\", \"prog_count\", \"rit_count\"]].apply(pd.to_numeric, downcast=\"unsigned\") # Convert to int dtype--holds positive numbers (no decimals)\n",
    "    \n",
    "    try:\n",
    "        # load error_file as a list with four pieces, the last element of each of which is the flag value itself:\n",
    "        df.loc[:,\"error_text\"] = df.loc[:,\"error_file\"].apply(lambda x: load_list('{}'.format(str(x))))\n",
    "        df.loc[:,\"duplicate_flag\"] = df.loc[:,\"error_text\"].apply(lambda x: '{}'.format(str(x[0].split()[-1]))) # int(df.error_text[0].split()[-1]) # last element of first piece of error_text\n",
    "        df.loc[:,\"parse_error_flag\"] = df.loc[:,\"error_text\"].apply(lambda x: '{}'.format(str(x[1].split()[-1]))) #int(df.error_text[1].split()[-1])\n",
    "        df.loc[:,\"wget_fail_flag\"] = df.loc[:,\"error_text\"].apply(lambda x: '{}'.format(str(x[2].split()[-1]))) #int(df.error_text[2].split()[-1])\n",
    "        df.loc[:,\"html_file_count\"] = df.loc[:,\"error_text\"].apply(lambda x: '{}'.format(str(x[3].split()[-1]))) #int(df.error_text[3].split()[-1])\n",
    "        \n",
    "        #if df[\"html_file_count\"]==0:\n",
    "        #    df[\"wget_fail_flag\"] = 1 # If no HTML, then web download failed! ## REDUNDANT with parse_school()\n",
    "        \n",
    "        #df['wget_fail_flag'] = df.folder_name.progress_apply(lambda x: set_failflag(x)) # Comment out while fixing parser\n",
    "        downloaded = df[\"wget_fail_flag\"].map({\"1\":True,1:True,\"0\":False,0:False}) == False # This binary conditional filters df to only those rows with downloaded web content (where wget_fail_flag==False and thus does NOT signal download failure)\n",
    "        \n",
    "        print(\"Loading webtext from disk into DF...\")\n",
    "        \n",
    "        # Load school parse output from disk into DataFrame:\n",
    "        # df.loc[:,(downloaded,\"keywords_text\")] = df.loc[:,(downloaded,\"school_folder\")].progress_apply...\n",
    "        df.loc[downloaded,\"webtext\"] = df.loc[downloaded,\"school_folder\"].progress_apply(lambda x: load_list(\"{}webtext.txt\".format(str(x)))) # df[\"wget_fail_flag\"]==False\n",
    "        df.loc[downloaded,\"keywords_text\"] = df.loc[downloaded,\"school_folder\"].progress_apply(lambda x: load_list(\"{}keywords_text.txt\".format(str(x))))\n",
    "        df.loc[downloaded,\"ideology_text\"] = df.loc[downloaded,\"school_folder\"].progress_apply(lambda x: load_list(\"{}ideology_text.txt\".format(str(x))))\n",
    "        \n",
    "        df[\"counts_text\"] = df.counts_file.apply(lambda x: load_list(\"{}\".format(str(x))))\n",
    "        df.loc[downloaded,\"ess_count\"] = df.loc[downloaded,\"counts_text\"].apply(lambda x: \"{}\".format(str(x[0].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 1st row in counts_text: take as uint dtype (no negatives)\n",
    "        df.loc[downloaded,\"prog_count\"] = df.loc[downloaded,\"counts_text\"].apply(lambda x: \"{}\".format(str(x[1].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 2nd row\n",
    "        df.loc[downloaded,\"rit_count\"] = df.loc[downloaded,\"counts_text\"].apply(lambda x: \"{}\".format(str(x[2].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 3nd row\n",
    "        df.loc[downloaded,\"ess_strength\"] = (df.loc[downloaded,\"ess_count\"]/df.loc[downloaded, \"rit_count\"]).apply(pd.to_numeric, downcast='float') # calculate ideology ratio, use most memory-efficient float dtype\n",
    "        df.loc[downloaded,\"prog_strength\"] = (df.loc[downloaded,\"prog_count\"]/df.loc[downloaded, \"rit_count\"]).apply(pd.to_numeric, downcast='float') \n",
    "        logging.info(str(df.loc[downloaded,'prog_strength']))\n",
    "        \n",
    "        print(\"Finished loading webtext into DF.\")\n",
    "        \n",
    "        df.drop([\"school_folder\",\"error_text\",\"error_file\",\"counts_text\"],axis=1) # Clean up temp variables\n",
    "        \n",
    "        #print(\"  LOADED \" + df[\"html_file_count\"].sum() + \" .html files from into DataFrame!\")\n",
    "        #save_datafile(df, save_dir+\"df_parser_temp\", \"pickle\") # Save output so we can pick up where left off, in case something breaks before able to save final output\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.critical(\"ERROR! Pandify function failed to load parsing output into DataFrame.\\n\" + str(e))\n",
    "        print(\"    ERROR! Pandify function failed to load parsing output into DataFrame.\")\n",
    "        print(\"  \",str(e))\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_pandify(bigdf, numsplits, df_filepath):\n",
    "    \"\"\"This function uses pandify_webtext() to load the parsing output from local storage into a DataFrame.\n",
    "    It gets around system memory limitations--which otherwise lead terminal to kill any attempts to pandify() all of bigdf--\n",
    "    by splitting bigdf into numsplits smaller dfslices, parsing webtext into each slice, and recombining them\n",
    "    by appending them to a big CSV on file. \n",
    "    The number of slices equals numsplits, and bigdf is split by numschools/ numsplits.\"\"\"\n",
    "    \n",
    "    global numschools # Access numschools from within function (this is roughly 7000)\n",
    "    wheresplit = int(round(float(numschools)/numsplits)) # Get number on which to split (e.g., 1000) based on total number of schools data. This splitting number will be iterated over using numsplits\n",
    "    \n",
    "    for num in range(numsplits): # tqdm(range(numsplits), desc=\"Loading dfslices\"): # Wrap iterator with tqdm to show progress bar\n",
    "        try:\n",
    "            dfslice = pd.DataFrame()\n",
    "            startnum, endnum = wheresplit*int(num),wheresplit*int(num+1)\n",
    "            dfslice = bigdf.iloc[startnum:endnum,:]\n",
    "            print(\"Loading into DataFrame parsing output for schools from \" + str(startnum) + \" to \" + str(endnum) + \" out of a total of \" + str(numschools) + \" school websites...\")\n",
    "            dfslice = pandify_webtext(dfslice) # Load parsed output into the DF\n",
    "            if num==1:\n",
    "                save_datafile(dfslice,df_filepath,\"CSV\") # Save this first chunk of results to new file, overwriting if needed\n",
    "            else:\n",
    "                dfslice.to_csv(df_filepath,mode=\"a\",index=False) # Append this next chunk of results to existing saved results\n",
    "                print(\"Data saved to \" + df_filepath + \"!\")\n",
    "            del dfslice # Free memory by deleting this temporary, smaller slice\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.critical(\"ERROR! Script failed to load parsing output into DataFrame slice #\" + str(num) + \" of \" + str(numsplits) + \".\\n\" + e)\n",
    "            print(\"  ERROR! Script failed to load parsing output into DataFrame slice #\" + str(num) + \" of \" + str(numsplits) + \".\")\n",
    "            print(\"  \",e)\n",
    "            sys.exit()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # REDUNDANT with parse_school\\nif __name__ == \\'__main__\\':\\n    with Pool(numcpus) as p: # Use multiprocessing.Pool(numcpus) to speed things up\\n        p.map(set_failflag, tqdm(folder_names, desc=\"Setting web_fail_flags\"), chunksize=numcpus)\\n        '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ### Preparing data to be parsed\n",
    "\n",
    "itervar = 0 # initialize iterator that counts number of schools already parsed--useless when multiprocessing\n",
    "parsed = [] # initialize list of URLs that have already been parsed\n",
    "dicts_list = [] # initialize list of dictionaries to hold school data\n",
    "schooldf = pd.DataFrame() # initialize DataFrame to hold school data\n",
    "\n",
    "# If input_file was defined by user input in beginning of script, use that to load list of dictionaries. We'll add to it!\n",
    "if usefile and not dicts_list:\n",
    "    dicts_list = load_datafile(input_file)\n",
    "    data_loc = full_schooldata # If loading data, assume we're running on full charter population\n",
    "\n",
    "else:\n",
    "    # set charter school data file and corresponding varnames:\n",
    "    \n",
    "    data_loc = full_schooldata # Run at scale using URL list of full charter population\n",
    "    # data_loc = micro_sample13 # This seems nice for debugging--except directories don't match because different data source\n",
    "        \n",
    "    # Create dict list from CSV on file, with one dict per school\n",
    "    with open(data_loc, 'r', encoding = 'Latin1') as csvfile: # open data file\n",
    "        reader = csv.DictReader(csvfile) # create a reader\n",
    "        for row in reader: # loop through rows\n",
    "            dicts_list.append(row) # append each row to the list\n",
    "        \n",
    "URL_var,NAME_var,ADDR_var = get_vars(data_loc) # get varnames depending on data source\n",
    "numschools = int(len(dicts_list)) # Count number of schools in list of dictionaries\n",
    "names,addresses,urls,folder_names = [[] for _ in range(4)]\n",
    "\n",
    "\n",
    "for school in dicts_list: # tqdm(dicts_list, desc=\"Setting web_fail_flags\"): # Wrap iterator with tqdm to show progress bar\n",
    "    names.append(school[NAME_var])\n",
    "    addresses.append(school[ADDR_var])\n",
    "    urls.append(school[URL_var])\n",
    "    school[\"folder_name\"] = re.sub(\" \",\"_\",(school[NAME_var]+\" \"+school[ADDR_var][-8:-6])) # This gives name and state separated by \"_\"\n",
    "    folder_names.append(school[\"folder_name\"])\n",
    "    # school['wget_fail_flag'] = set_failflag(school[\"folder_name\"]) # REDUNDANT with parse_school()\n",
    "    # save_datafile(dicts_list, temp_dir+\"school_parser_temp\", \"JSON\") # Save output so we can pick up where left off, in case something breaks\n",
    "    \n",
    "tuplist_zip = zip(names, addresses, urls, folder_names) # Create list of tuples to pass to parser function\n",
    "\n",
    "\"\"\" # REDUNDANT with parse_school\n",
    "if __name__ == '__main__':\n",
    "    with Pool(numcpus) as p: # Use multiprocessing.Pool(numcpus) to speed things up\n",
    "        p.map(set_failflag, tqdm(folder_names, desc=\"Setting web_fail_flags\"), chunksize=numcpus)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Skip to main content\\n', 'State of Alaska\\n', 'myAlaska\\n', 'My Government\\n', 'Resident\\n', 'Business in Alaska\\n', 'Visiting Alaska\\n', 'State Employees\\n', 'Department of Education & Early Development\\n', 'EED Website\\n', 'State of Alaska\\n', 'Home\\n', 'Parents & Students\\n', 'Teaching & Learning\\n', 'Forms & Grants\\n', 'Finance & Facilities\\n', 'Statistics & Reports\\n', 'About EED\\n', 'State of Alaska\\n', ' > \\n', 'DEED\\n', ' > \\n', 'Alaska Public Schools Database\\n', '                    > Ayaprun Elitnaurvik            \\n', 'Ayaprun Elitnaurvik\\n', 'Lower Kuskokwim School District\\n', 'School Calendar for 2017-2018\\n', 'School ID\\n', '319010\\n', 'Address\\n', 'PO Box 1468\\n', 'Bethel, AK 99559\\n', 'Physical Address\\n', '1010 Fourth Ave\\n', 'Bethel, AK 99559\\n', 'Telephone\\n', '(907) 543-1645\\n', 'Fax\\n', '         (907) 543-1647\\n', 'School Website\\n', 'Ayaprun Elitnaurvik\\n', 'School Email\\n', 'Contact Name\\n', 'Sam Crow\\n', ', Principal\\n', 'Lowest Grade\\n', 'PK\\n', 'Highest Grade\\n', '6\\n', 'Enrollment\\n', '2016-2017 Enrollment: 163\\n', 'Home\\n', 'School Details\\n', 'Website Information\\n', 'State of Alaska Homepage\\n', 'Contact Information\\n', '                    Alaska Dept. of Education                    \\n', '                    & Early Development                \\n', '                    801 West 10th Street, Suite 200                    \\n', '                    PO Box 110500                    \\n', '                    Juneau, AK 99811-0500                \\n', '                    Telephone: \\n', '(907) 465-2800\\n', '                    Teacher Certification: \\n', '(907) 465-2831\\n', '                    TTY/TTD: \\n', '(907) 465-2815\\n', '                    Fax: (907) 465-4156                \\n', 'eed.webmaster@alaska.gov\\n', 'Teaching & Learning Support Program Contacts\\n', 'More. . .\\n', 'Department Links\\n', 'Alaska State Council on the Arts\\n', 'Alaska Commission on Postsecondary Education\\n', 'Professional Teaching Practices Commission\\n', 'Review and Comment on Proposed Regulation\\n', 'State Board of Education & Early Development\\n', 'Teacher Certification\\n', 'State of Alaska\\n', 'myAlaska\\n', 'My Government\\n', 'Resident\\n', 'Business in Alaska\\n', 'Visiting Alaska\\n', 'State Employees\\n', 'State of Alaska\\n', '© 2013\\n', 'Webmaster\\n']\n"
     ]
    }
   ],
   "source": [
    "schooldf = pd.DataFrame(dicts_list)\n",
    "thislist = load_list(wget_dataloc + 'Ayaprun_Elitnaurvik_AK/webtext.txt')\n",
    "print(thislist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nempty = [\"\" for elem in range(len(schooldf[\"NCESSCH\"]))] # Create empty column length of the longest variable\\nschooldf = schooldf.assign(webtext=empty, keywords_text=empty, ideology_text=empty, ess_count=empty, prog_count=empty, rit_count=empty)\\nprint(len(empty_column))\\n\\nschooldf = schooldf.assign(keywords_text = empty_column, webtext = empty_column)\\nschooldf[\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\\nprint(type(schooldf.loc[0,\"keywords_text\"]), type(schooldf), type(schooldf.keywords_text))\\nprint()\\nprint(schooldf)\\n\\n#schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]] = schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]].astype(object)\\n#schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]] = schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]].apply(pd.to_numeric,downcast=\\'float\\')\\n\\n#schooldf.at[:,\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\\n#schooldf.at[:,\"keywords_text\"] = []\\n#schooldf.at[0,\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\\n# schooldf = schooldf.assign(\"ess_count\")\\n#schooldf[\"ess_count\"] = schooldf[\"ess_count\"].apply(pd.to_numeric)\\n\\n# converted_df.loc[:,col] = df[col].astype(\\'category\\')\\n# converted_df.select_dtypes(include=[\\'float\\']).apply(pd.to_numeric,downcast=\\'float\\')\\n\\n#.astype(object), df[\"keywords_text\"].astype(object), df[\"ideology_text\"].astype(object), df[\"ess_count\"].astype(int), df[\"prog_count\"].astype(int), df[\"rit_count\"].astype(int)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "empty = [\"\" for elem in range(len(schooldf[\"NCESSCH\"]))] # Create empty column length of the longest variable\n",
    "schooldf = schooldf.assign(webtext=empty, keywords_text=empty, ideology_text=empty, ess_count=empty, prog_count=empty, rit_count=empty)\n",
    "print(len(empty_column))\n",
    "\n",
    "schooldf = schooldf.assign(keywords_text = empty_column, webtext = empty_column)\n",
    "schooldf[\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\n",
    "print(type(schooldf.loc[0,\"keywords_text\"]), type(schooldf), type(schooldf.keywords_text))\n",
    "print()\n",
    "print(schooldf)\n",
    "\n",
    "#schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]] = schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]].astype(object)\n",
    "#schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]] = schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]].apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "#schooldf.at[:,\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\n",
    "#schooldf.at[:,\"keywords_text\"] = []\n",
    "#schooldf.at[0,\"keywords_text\"] = schooldf[\"keywords_text\"].astype(object)\n",
    "# schooldf = schooldf.assign(\"ess_count\")\n",
    "#schooldf[\"ess_count\"] = schooldf[\"ess_count\"].apply(pd.to_numeric)\n",
    "\n",
    "# converted_df.loc[:,col] = df[col].astype('category')\n",
    "# converted_df.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "#.astype(object), df[\"keywords_text\"].astype(object), df[\"ideology_text\"].astype(object), df[\"ess_count\"].astype(int), df[\"prog_count\"].astype(int), df[\"rit_count\"].astype(int)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6972 entries, 0 to 6971\n",
      "Columns: 187 entries, SURVYEAR to webtext\n",
      "dtypes: object(187)\n",
      "memory usage: 79.3 MB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6972 entries, 0 to 6971\n",
      "Columns: 187 entries, SURVYEAR to webtext\n",
      "dtypes: category(155), float32(3), object(29)\n",
      "memory usage: 19.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize text strings and counts as empty, then convert to more efficient data types:\n",
    "empty = [\"\" for elem in range(len(schooldf[\"NCESSCH\"]))] # Create empty string column length of longest variable (NCESCCH used for matching)\n",
    "schooldf = schooldf.assign(webtext=empty, keywords_text=empty, ideology_text=empty, ess_count=empty, prog_count=empty, rit_count=empty) # Add empty columns to df\n",
    "print(schooldf.info(memory_usage='deep'))\n",
    "\n",
    "schooldf = convert_df(schooldf)\n",
    "schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]] = schooldf[[\"webtext\", \"keywords_text\", \"ideology_text\"]].apply(lambda x: x.astype(object)) # Convert to string type (to be safe)\n",
    "schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]] = schooldf[[\"ess_count\", \"prog_count\", \"rit_count\"]].apply(pd.to_numeric,downcast='float')  # Convert to float type (if appropriate)\n",
    "\n",
    "print()\n",
    "print(schooldf.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#schooldf.iloc[0][\"webtext\"] = load_list(wget_dataloc + 'Ayaprun_Elitnaurvik_AK/webtext.txt') #schooldf.iloc[0].apply(lambda x: load_list(wget_dataloc + \"{}/webtext.txt\".format(str(x.folder_name))))\n",
    "#schooldf[\"webtext\"] = schooldf[\"webtext\"].astype(object)\n",
    "#schooldf.at[0,\"webtext\"] = thislist\n",
    "#schooldf.loc[0,\"webtext\"]\n",
    "#schooldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "schooldf[\"school_folder\"] = schooldf.folder_name.apply(lambda x: str(wget_dataloc) + '{}/'.format(str(x)))\n",
    "schooldf[\"error_file\"] = schooldf.school_folder.apply(lambda x: '{}error_flags.txt'.format(str(x))) # Define file path for error text log\n",
    "schooldf[\"counts_file\"] = schooldf.school_folder.apply(lambda x: '{}dict_counts.txt'.format(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_folder</th>\n",
       "      <th>error_file</th>\n",
       "      <th>counts_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ayaprun_Elit...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ayaprun_Elit...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ayaprun_Elit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ketchikan_Ch...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ketchikan_Ch...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Ketchikan_Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Tongass_Scho...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Tongass_Scho...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Tongass_Scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aquarian_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aquarian_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aquarian_Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Family_Partn...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Family_Partn...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Family_Partn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Winterberry_...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Winterberry_...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Winterberry_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Eagle_Academ...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Eagle_Academ...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Eagle_Academ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Frontier_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Frontier_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Frontier_Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Highland_Tec...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Highland_Tec...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Highland_Tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Rilke_Schule...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Rilke_Schule...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Rilke_Schule...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Alaska_Nativ...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Alaska_Nativ...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Alaska_Nativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Juneau_Commu...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Juneau_Commu...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Juneau_Commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aurora_Borea...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aurora_Borea...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Aurora_Borea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fireweed_Aca...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fireweed_Aca...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fireweed_Aca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Soldotna_Mon...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Soldotna_Mon...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Soldotna_Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Kaleidoscope...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Kaleidoscope...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Kaleidoscope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Academy_Char...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Academy_Char...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Academy_Char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Midnight_Sun...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Midnight_Sun...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Midnight_Sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/American_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/American_Cha...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/American_Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Twindly_Brid...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Twindly_Brid...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Twindly_Brid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fronteras_Ch...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fronteras_Ch...</td>\n",
       "      <td>/home/jovyan/work/wget/parll_wget/Fronteras_Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        school_folder  \\\n",
       "0   /home/jovyan/work/wget/parll_wget/Ayaprun_Elit...   \n",
       "1   /home/jovyan/work/wget/parll_wget/Ketchikan_Ch...   \n",
       "2   /home/jovyan/work/wget/parll_wget/Tongass_Scho...   \n",
       "3   /home/jovyan/work/wget/parll_wget/Aquarian_Cha...   \n",
       "4   /home/jovyan/work/wget/parll_wget/Family_Partn...   \n",
       "5   /home/jovyan/work/wget/parll_wget/Winterberry_...   \n",
       "6   /home/jovyan/work/wget/parll_wget/Eagle_Academ...   \n",
       "7   /home/jovyan/work/wget/parll_wget/Frontier_Cha...   \n",
       "8   /home/jovyan/work/wget/parll_wget/Highland_Tec...   \n",
       "9   /home/jovyan/work/wget/parll_wget/Rilke_Schule...   \n",
       "10  /home/jovyan/work/wget/parll_wget/Alaska_Nativ...   \n",
       "11  /home/jovyan/work/wget/parll_wget/Juneau_Commu...   \n",
       "12  /home/jovyan/work/wget/parll_wget/Aurora_Borea...   \n",
       "13  /home/jovyan/work/wget/parll_wget/Fireweed_Aca...   \n",
       "14  /home/jovyan/work/wget/parll_wget/Soldotna_Mon...   \n",
       "15  /home/jovyan/work/wget/parll_wget/Kaleidoscope...   \n",
       "16  /home/jovyan/work/wget/parll_wget/Academy_Char...   \n",
       "17  /home/jovyan/work/wget/parll_wget/Midnight_Sun...   \n",
       "18  /home/jovyan/work/wget/parll_wget/American_Cha...   \n",
       "19  /home/jovyan/work/wget/parll_wget/Twindly_Brid...   \n",
       "20  /home/jovyan/work/wget/parll_wget/Fronteras_Ch...   \n",
       "\n",
       "                                           error_file  \\\n",
       "0   /home/jovyan/work/wget/parll_wget/Ayaprun_Elit...   \n",
       "1   /home/jovyan/work/wget/parll_wget/Ketchikan_Ch...   \n",
       "2   /home/jovyan/work/wget/parll_wget/Tongass_Scho...   \n",
       "3   /home/jovyan/work/wget/parll_wget/Aquarian_Cha...   \n",
       "4   /home/jovyan/work/wget/parll_wget/Family_Partn...   \n",
       "5   /home/jovyan/work/wget/parll_wget/Winterberry_...   \n",
       "6   /home/jovyan/work/wget/parll_wget/Eagle_Academ...   \n",
       "7   /home/jovyan/work/wget/parll_wget/Frontier_Cha...   \n",
       "8   /home/jovyan/work/wget/parll_wget/Highland_Tec...   \n",
       "9   /home/jovyan/work/wget/parll_wget/Rilke_Schule...   \n",
       "10  /home/jovyan/work/wget/parll_wget/Alaska_Nativ...   \n",
       "11  /home/jovyan/work/wget/parll_wget/Juneau_Commu...   \n",
       "12  /home/jovyan/work/wget/parll_wget/Aurora_Borea...   \n",
       "13  /home/jovyan/work/wget/parll_wget/Fireweed_Aca...   \n",
       "14  /home/jovyan/work/wget/parll_wget/Soldotna_Mon...   \n",
       "15  /home/jovyan/work/wget/parll_wget/Kaleidoscope...   \n",
       "16  /home/jovyan/work/wget/parll_wget/Academy_Char...   \n",
       "17  /home/jovyan/work/wget/parll_wget/Midnight_Sun...   \n",
       "18  /home/jovyan/work/wget/parll_wget/American_Cha...   \n",
       "19  /home/jovyan/work/wget/parll_wget/Twindly_Brid...   \n",
       "20  /home/jovyan/work/wget/parll_wget/Fronteras_Ch...   \n",
       "\n",
       "                                          counts_file  \n",
       "0   /home/jovyan/work/wget/parll_wget/Ayaprun_Elit...  \n",
       "1   /home/jovyan/work/wget/parll_wget/Ketchikan_Ch...  \n",
       "2   /home/jovyan/work/wget/parll_wget/Tongass_Scho...  \n",
       "3   /home/jovyan/work/wget/parll_wget/Aquarian_Cha...  \n",
       "4   /home/jovyan/work/wget/parll_wget/Family_Partn...  \n",
       "5   /home/jovyan/work/wget/parll_wget/Winterberry_...  \n",
       "6   /home/jovyan/work/wget/parll_wget/Eagle_Academ...  \n",
       "7   /home/jovyan/work/wget/parll_wget/Frontier_Cha...  \n",
       "8   /home/jovyan/work/wget/parll_wget/Highland_Tec...  \n",
       "9   /home/jovyan/work/wget/parll_wget/Rilke_Schule...  \n",
       "10  /home/jovyan/work/wget/parll_wget/Alaska_Nativ...  \n",
       "11  /home/jovyan/work/wget/parll_wget/Juneau_Commu...  \n",
       "12  /home/jovyan/work/wget/parll_wget/Aurora_Borea...  \n",
       "13  /home/jovyan/work/wget/parll_wget/Fireweed_Aca...  \n",
       "14  /home/jovyan/work/wget/parll_wget/Soldotna_Mon...  \n",
       "15  /home/jovyan/work/wget/parll_wget/Kaleidoscope...  \n",
       "16  /home/jovyan/work/wget/parll_wget/Academy_Char...  \n",
       "17  /home/jovyan/work/wget/parll_wget/Midnight_Sun...  \n",
       "18  /home/jovyan/work/wget/parll_wget/American_Cha...  \n",
       "19  /home/jovyan/work/wget/parll_wget/Twindly_Brid...  \n",
       "20  /home/jovyan/work/wget/parll_wget/Fronteras_Ch...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schooldf.loc[:20,[\"school_folder\", \"error_file\", \"counts_file\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SURVYEAR</th>\n",
       "      <th>FIPST</th>\n",
       "      <th>STABR</th>\n",
       "      <th>STATENAME</th>\n",
       "      <th>SEANAME</th>\n",
       "      <th>LEAID</th>\n",
       "      <th>ST_LEAID</th>\n",
       "      <th>LEA_NAME</th>\n",
       "      <th>SCHID</th>\n",
       "      <th>ST_SCHID</th>\n",
       "      <th>...</th>\n",
       "      <th>folder_name</th>\n",
       "      <th>ess_count</th>\n",
       "      <th>ideology_text</th>\n",
       "      <th>keywords_text</th>\n",
       "      <th>prog_count</th>\n",
       "      <th>rit_count</th>\n",
       "      <th>webtext</th>\n",
       "      <th>school_folder</th>\n",
       "      <th>error_file</th>\n",
       "      <th>counts_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SURVYEAR, FIPST, STABR, STATENAME, SEANAME, LEAID, ST_LEAID, LEA_NAME, SCHID, ST_SCHID, NCESSCH, SCH_NAME, MSTREET1, MSTREET2, MSTREET3, MCITY, MSTATE, MZIP, MZIP4, PHONE, LSTREET114, LSTREET214, LSTREET314, LCITY14, LSTATE14, LZIP14, LZIP4, UNION, OUT_OF_STATE_FLAG, SCH_TYPE_TEXT, SCH_TYPE, RECON_STATUS, GSLO, GSHI, LEVEL, VIRTUAL, BIES, SY_STATUS_TEXT, SY_STATUS, UPDATED_STATUS_TEXT, UPDATED_STATUS, EFFECTIVE_DATE, CHARTER_TEXT, G13OFFERED, AEOFFERED, UGOFFERED, NOGRADES, CHARTAUTH1, CHARTAUTHN1, CHARTAUTH2, CHARTAUTHN2, IGOFFERED, FRELCH, ISFLE, REDLCH, TOTFRL, AE, AM, AMALF, AMALM, AS, ASALF, ASALM, BL, BLALF, BLALM, HI, HIALF, HIALM, HP, HPALF, HPALM, MEMBER, TOTAL, TR, TRALF, TRALM, WH, WHALF, WHALM, MAGNET_TEXT, NSLPSTATUS_CODE, NSLPSTATUS_TEXT, SHARED_TIME, STITLEI, TITLEI, TITLEI_STATUS, TITLEI_TEXT, FTE, ISFTEPUP, SCHNAM98, CHARTR98, STATUS98, SCHNAM99, CHARTR99, STATUS99, SCHNAM00, CHARTR00, STATUS00, SCHNAM01, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 190 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schooldf[schooldf[\"error_file\"]=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Loading DF\") # To show progress, create & register new `tqdm` instance with `pandas`\n",
    "dfslice = schooldf.iloc[0:30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 2532.84it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "1     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "2     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "3     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "4     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "5     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "6     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "7     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "8     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "9     [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "10    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "11    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "12    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "13    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "14    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "15    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "16    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "17    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "18    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "19    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "20    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "21    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "22    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "23    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "24    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "25    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "26    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "27    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "28    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "29    [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...\n",
       "Name: error_text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfslice[\"error_text\"] = dfslice.error_file.progress_apply(lambda x: load_list('{}'.format(str(x))))\n",
    "dfslice[\"error_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 14840.09it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 13775.91it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 17544.50it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 16324.48it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duplicate_flag</th>\n",
       "      <th>parse_error_flag</th>\n",
       "      <th>wget_fail_flag</th>\n",
       "      <th>html_file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duplicate_flag parse_error_flag wget_fail_flag html_file_count\n",
       "0               0                0              0               1\n",
       "1               0                0              0              61\n",
       "2               0                0              0               1\n",
       "3               0                0              0              49\n",
       "4               0                0              0               1\n",
       "5               0                0              0               1\n",
       "6               0                0              0               5\n",
       "7               0                0              0              25\n",
       "8               0                0              0              33\n",
       "9               0                0              0               1\n",
       "10              0                0              0              35\n",
       "11              0                0              0              11\n",
       "12              0                0              0               1\n",
       "13              0                0              0             273\n",
       "14              0                0              0              23\n",
       "15              0                0              0              87\n",
       "16              0                0              0              61\n",
       "17              0                0              0               3\n",
       "18              0                0              0               1\n",
       "19              0                0              0               1\n",
       "20              0                0              0              73\n",
       "21              0                0              0              85\n",
       "22              0                0              0              31\n",
       "23              0                0              0             105\n",
       "24              0                0              0             177\n",
       "25              0                0              1               0\n",
       "26              0                0              1               0\n",
       "27              0                0              1               0\n",
       "28              0                0              0              53\n",
       "29              0                0              0               1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfslice[\"duplicate_flag\"] = dfslice.error_text.progress_apply(lambda x: '{}'.format(str(x[0].split()[-1]))) # int(df.error_text[0].split()[-1]) # last element of first piece of error_text\n",
    "dfslice[\"parse_error_flag\"] = dfslice.error_text.progress_apply(lambda x: '{}'.format(str(x[1].split()[-1]))) #int(df.error_text[1].split()[-1])\n",
    "dfslice[\"wget_fail_flag\"] = dfslice.error_text.progress_apply(lambda x: '{}'.format(str(x[2].split()[-1]))) #int(df.error_text[2].split()[-1])\n",
    "dfslice[\"html_file_count\"] = dfslice.error_text.progress_apply(lambda x: '{}'.format(str(x[3].split()[-1]))) #int(df.error_text[3].split()[-1])\n",
    "\n",
    "dfslice[[\"duplicate_flag\", \"parse_error_flag\", \"wget_fail_flag\", \"html_file_count\"]]\n",
    "\n",
    "#if df[\"html_file_count\"]==0:\n",
    "#    df[\"wget_fail_flag\"] = 1 # If no HTML, then web download failed! ## REDUNDANT with parse_school()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['wget_fail_flag'] = df.folder_name.progress_apply(lambda x: set_failflag(x)) # Comment out while fixing parser\n",
    "downloaded = dfslice[\"wget_fail_flag\"].map({\"1\":True,1:True,\"0\":False,0:False}) == False # This binary conditional filters df to only those rows with downloaded web content (where wget_fail_flag==False and thus does NOT signal download failure)\n",
    "len(dfslice[downloaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 258.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     [Skip to main content\\n, State of Alaska\\n, my...\n",
       "1     [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "2     [Tongass School of Arts and Sciences\\n, Home\\n...\n",
       "3     [Main\\n, Main\\n, Anchorage school district\\n, ...\n",
       "4     [Skip to main content\\n, State of Alaska\\n, my...\n",
       "5     [Menu\\n, Home\\n, About\\n, Mission\\n, Leadershi...\n",
       "6     [Main\\n, Anchorage School District\\n, Educatin...\n",
       "7     [Home\\n, APC\\n, Business\\n, Forms\\n, Handbooks...\n",
       "8     [Main\\n, Main\\n, Anchorage school district\\n, ...\n",
       "9     [Skip to main content\\n, State of Alaska\\n, my...\n",
       "10    [Main\\n, Anchorage school district\\n, ASD Depa...\n",
       "11    [Juneau Community Charter School \\n, JCCS \\n, ...\n",
       "12    [Skip to main content\\n, State of Alaska\\n, my...\n",
       "13    [Fireweed Academy\\n, 995 Soundview Ave. Homer,...\n",
       "14    [Soldotna Elementary School\\n, 162 Park Ave. S...\n",
       "15    [Kaleidoscope School of Arts and Science\\n, 54...\n",
       "16    [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "17    [Home\\n, Classroom Pages \\n, ~ Mrs. MacDonald\\...\n",
       "18    [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "19    [\n",
       "Å§\u0005Jù ̈ï ́<Ö\u0002 ̧(­ðíwö(Z.`lÄa\u0000\n",
       "Ã*âGz\u000e<ÏN\u001aQÐ...\n",
       "20    [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "21    [Skip to content\\n, About\\n, Mission\\n, Histor...\n",
       "22    [Home\\n, About Us\\n, Program Description\\n, Sc...\n",
       "23    [Ø'TVÒ8n°èM¤·ÿ\u0007Í1\u0002C<\u00031n\u0015sU0<Q\u0002\n",
       "èæLÌ\u000fëc-3Î...\n",
       "24    [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "25                                                     \n",
       "26                                                     \n",
       "27                                                     \n",
       "28    [\u000f ̄ö-`ùÅ+KÝZ$QÙEP3⁄43ÇÂ1⁄2\u0014Z@ý\u0002a?éGoÌy6?~ ́...\n",
       "29    [Skip to Main Content\\n, District Home\\n, Sele...\n",
       "Name: webtext, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load school parse output from disk into DataFrame:\n",
    "# df.loc[:,(downloaded,\"keywords_text\")] = df.loc[:,(downloaded,\"school_folder\")].progress_apply...\n",
    "dfslice.loc[downloaded,\"webtext\"] = dfslice[downloaded].school_folder.progress_apply(lambda x: load_list(\"{}webtext.txt\".format(str(x)))) # df[\"wget_fail_flag\"]==False\n",
    "dfslice[\"webtext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 691.35it/s]\u001b[A\u001b[A/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "\n",
      "\n",
      "Loading DF:   0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF:  44%|████▍     | 12/27 [00:00<00:00, 105.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF:  67%|██████▋   | 18/27 [00:00<00:00, 85.41it/s] \u001b[A\u001b[A\n",
      "\n",
      "Loading DF:  89%|████████▉ | 24/27 [00:00<00:00, 72.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 50.53it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords_text</th>\n",
       "      <th>ideology_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Alaska Public Schools Database\\n, Lower Kusko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[KCS School Creed\\n, KCS School Creed\\n, KCS S...</td>\n",
       "      <td>[Select a School\\n, Ketchikan High School\\n, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Our Vision\\n, mission statement\\n, Our School...</td>\n",
       "      <td>[Tongass School of Arts and Sciences\\n, Daily ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Aquarian students explore topics outside of s...</td>\n",
       "      <td>[Anchorage school district\\n, School board\\n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Alaska Public Schools Database\\n,            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Mission\\n, Curriculum\\n]</td>\n",
       "      <td>[Mission\\n, Program\\n, What Is Waldorf Educati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Eagle Academy provides students with an excel...</td>\n",
       "      <td>[Anchorage School District\\n, Educating All St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[for home school students. Frontier is disting...</td>\n",
       "      <td>[New parent orientation meetings are held on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Curriculum\\n, Curriculum\\n, Curriculum\\n, Cur...</td>\n",
       "      <td>[Anchorage school district\\n, School board\\n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Alaska Public Schools Database\\n,            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ANCCS Curriculum \\n, Outdoor education, art, ...</td>\n",
       "      <td>[Anchorage school district\\n, School board\\n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Our Charter allows us the flexibility to cust...</td>\n",
       "      <td>[Juneau Community Charter School \\n, Juneau Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Alaska Public Schools Database\\n,            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[MISSION STATEMENT\\n, The Kenai Peninsula Boro...</td>\n",
       "      <td>[Prior Year APC Minutes\\n, FWA- a slice of lif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Vision Statement\\n, Vision Statement\\n, Visio...</td>\n",
       "      <td>[Soldotna Elementary School\\n, School Newslett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[ background check through the district websit...</td>\n",
       "      <td>[Kaleidoscope School of Arts and Science\\n, Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[Do you believe in school choice??? Do you wan...</td>\n",
       "      <td>[Select a School\\n, Academy Charter\\n, America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[~ Mission Statement\\n, ",
       "¢8È÷ôàçå\\É~!þþO...</td>\n",
       "      <td>[About         Our School\\n, ~ Mission Stateme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[Mrs. Brady is the designer and creator of the...</td>\n",
       "      <td>[Select a School\\n, Academy Charter\\n, America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[]</td>\n",
       "      <td>[\u001e",
       "Å§\u0005Jù ̈ï ́&lt;Ö\u0002 ̧(­ðíwö(Z.`lÄa\u0000\u001d",
       "Ã*âGz\u000e&lt;ÏN\u001aQÐ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[other forms  can be found on the Fronteras on...</td>\n",
       "      <td>[Select a School\\n, Academy Charter\\n, America...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[Mission\\n, Curriculum\\n, Birchtree Charter Sc...</td>\n",
       "      <td>[Mission\\n, School Hours/Transportation\\n, Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[]</td>\n",
       "      <td>[Program Description\\n, School Goals\\n, Progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[The Extended Learning Program (ELP), formerly...</td>\n",
       "      <td>[Ø'TVÒ8n°èM¤·ÿ\u0007Í1\u0002C&lt;\u00031n\u0015sU0&lt;Q\u0002\u000b",
       "èæLÌ\u000fëc-3Î...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[The middle school program creates small learn...</td>\n",
       "      <td>[Select a School\\n, Ben Eielson Jr./Sr. High\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Í©°òI©\u0014uÆ£Ê)âô®«\\n, Ce,Ö6\u0003à#l-èRvq4TEð\"øû\u0011...</td>\n",
       "      <td>[\u000f ̄ö-`ùÅ+KÝZ$QÙEP3⁄43ÇÂ1⁄2\u0014Z@ý\u0002a?éGoÌy6?~ ́...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[Mission Statement\\n]</td>\n",
       "      <td>[Select a School\\n, Cactus Shadows High School...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        keywords_text  \\\n",
       "0                                                  []   \n",
       "1   [KCS School Creed\\n, KCS School Creed\\n, KCS S...   \n",
       "2   [Our Vision\\n, mission statement\\n, Our School...   \n",
       "3   [Aquarian students explore topics outside of s...   \n",
       "4                                                  []   \n",
       "5                           [Mission\\n, Curriculum\\n]   \n",
       "6   [Eagle Academy provides students with an excel...   \n",
       "7   [for home school students. Frontier is disting...   \n",
       "8   [Curriculum\\n, Curriculum\\n, Curriculum\\n, Cur...   \n",
       "9                                                  []   \n",
       "10  [ANCCS Curriculum \\n, Outdoor education, art, ...   \n",
       "11  [Our Charter allows us the flexibility to cust...   \n",
       "12                                                 []   \n",
       "13  [MISSION STATEMENT\\n, The Kenai Peninsula Boro...   \n",
       "14  [Vision Statement\\n, Vision Statement\\n, Visio...   \n",
       "15  [ background check through the district websit...   \n",
       "16  [Do you believe in school choice??? Do you wan...   \n",
       "17  [~ Mission Statement\\n, \n",
       "¢8È÷ôàçå\\É~!þþO...   \n",
       "18  [Mrs. Brady is the designer and creator of the...   \n",
       "19                                                 []   \n",
       "20  [other forms  can be found on the Fronteras on...   \n",
       "21  [Mission\\n, Curriculum\\n, Birchtree Charter Sc...   \n",
       "22                                                 []   \n",
       "23  [The Extended Learning Program (ELP), formerly...   \n",
       "24  [The middle school program creates small learn...   \n",
       "28  [Í©°òI©\u0014uÆ£Ê)âô®«\\n, Ce,Ö6\u0003à#l-èRvq4TEð\"øû\u0011...   \n",
       "29                              [Mission Statement\\n]   \n",
       "\n",
       "                                        ideology_text  \n",
       "0   [Alaska Public Schools Database\\n, Lower Kusko...  \n",
       "1   [Select a School\\n, Ketchikan High School\\n, K...  \n",
       "2   [Tongass School of Arts and Sciences\\n, Daily ...  \n",
       "3   [Anchorage school district\\n, School board\\n, ...  \n",
       "4   [Alaska Public Schools Database\\n,            ...  \n",
       "5   [Mission\\n, Program\\n, What Is Waldorf Educati...  \n",
       "6   [Anchorage School District\\n, Educating All St...  \n",
       "7   [New parent orientation meetings are held on t...  \n",
       "8   [Anchorage school district\\n, School board\\n, ...  \n",
       "9   [Alaska Public Schools Database\\n,            ...  \n",
       "10  [Anchorage school district\\n, School board\\n, ...  \n",
       "11  [Juneau Community Charter School \\n, Juneau Co...  \n",
       "12  [Alaska Public Schools Database\\n,            ...  \n",
       "13  [Prior Year APC Minutes\\n, FWA- a slice of lif...  \n",
       "14  [Soldotna Elementary School\\n, School Newslett...  \n",
       "15  [Kaleidoscope School of Arts and Science\\n, Ch...  \n",
       "16  [Select a School\\n, Academy Charter\\n, America...  \n",
       "17  [About         Our School\\n, ~ Mission Stateme...  \n",
       "18  [Select a School\\n, Academy Charter\\n, America...  \n",
       "19  [\n",
       "Å§\u0005Jù ̈ï ́<Ö\u0002 ̧(­ðíwö(Z.`lÄa\u0000\n",
       "Ã*âGz\u000e<ÏN\u001aQÐ...  \n",
       "20  [Select a School\\n, Academy Charter\\n, America...  \n",
       "21  [Mission\\n, School Hours/Transportation\\n, Pro...  \n",
       "22  [Program Description\\n, School Goals\\n, Progra...  \n",
       "23  [Ø'TVÒ8n°èM¤·ÿ\u0007Í1\u0002C<\u00031n\u0015sU0<Q\u0002\n",
       "èæLÌ\u000fëc-3Î...  \n",
       "24  [Select a School\\n, Ben Eielson Jr./Sr. High\\n...  \n",
       "28  [\u000f ̄ö-`ùÅ+KÝZ$QÙEP3⁄43ÇÂ1⁄2\u0014Z@ý\u0002a?éGoÌy6?~ ́...  \n",
       "29  [Select a School\\n, Cactus Shadows High School...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfslice.loc[downloaded,\"keywords_text\"] = dfslice.loc[downloaded,\"school_folder\"].progress_apply(lambda x: load_list(\"{}keywords_text.txt\".format(str(x))))\n",
    "dfslice.loc[downloaded,\"ideology_text\"] = dfslice.loc[downloaded,\"school_folder\"].progress_apply(lambda x: load_list(\"{}ideology_text.txt\".format(str(x))))\n",
    "dfslice.loc[downloaded,[\"keywords_text\",\"ideology_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DF: 100%|██████████| 30/30 [00:00<00:00, 4444.53it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 49560.70it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 51428.80it/s]\n",
      "Loading DF: 100%|██████████| 27/27 [00:00<00:00, 61714.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts_text</th>\n",
       "      <th>ess_count</th>\n",
       "      <th>prog_count</th>\n",
       "      <th>rit_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 29]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ess_count 45\\n, prog_count 116\\n, rit_count 1...</td>\n",
       "      <td>45</td>\n",
       "      <td>116</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ess_count 3\\n, prog_count 7\\n, rit_count 80]</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ess_count 93\\n, prog_count 85\\n, rit_count 975]</td>\n",
       "      <td>93</td>\n",
       "      <td>85</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ess_count 1\\n, prog_count 8\\n, rit_count 25]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ess_count 50\\n, prog_count 17\\n, rit_count 167]</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[ess_count 129\\n, prog_count 84\\n, rit_count 763]</td>\n",
       "      <td>129</td>\n",
       "      <td>84</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ess_count 89\\n, prog_count 60\\n, rit_count 774]</td>\n",
       "      <td>89</td>\n",
       "      <td>60</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[ess_count 116\\n, prog_count 158\\n, rit_count ...</td>\n",
       "      <td>116</td>\n",
       "      <td>158</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[ess_count 31\\n, prog_count 120\\n, rit_count 527]</td>\n",
       "      <td>31</td>\n",
       "      <td>120</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[ess_count 36\\n, prog_count 901\\n, rit_count 2...</td>\n",
       "      <td>36</td>\n",
       "      <td>901</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[ess_count 1\\n, prog_count 7\\n, rit_count 192]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[ess_count 104\\n, prog_count 391\\n, rit_count ...</td>\n",
       "      <td>104</td>\n",
       "      <td>391</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[ess_count 112\\n, prog_count 120\\n, rit_count ...</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>2326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[ess_count 10\\n, prog_count 8\\n, rit_count 44]</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[ess_count 5\\n, prog_count 12\\n, rit_count 162]</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[ess_count 10\\n, prog_count 0\\n, rit_count 40]</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[ess_count 131\\n, prog_count 134\\n, rit_count ...</td>\n",
       "      <td>131</td>\n",
       "      <td>134</td>\n",
       "      <td>2556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[ess_count 126\\n, prog_count 507\\n, rit_count ...</td>\n",
       "      <td>126</td>\n",
       "      <td>507</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[ess_count 47\\n, prog_count 85\\n, rit_count 300]</td>\n",
       "      <td>47</td>\n",
       "      <td>85</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[ess_count 237\\n, prog_count 276\\n, rit_count ...</td>\n",
       "      <td>237</td>\n",
       "      <td>276</td>\n",
       "      <td>3233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[ess_count 653\\n, prog_count 489\\n, rit_count ...</td>\n",
       "      <td>653</td>\n",
       "      <td>489</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[ess_count 126\\n, prog_count 36\\n, rit_count 640]</td>\n",
       "      <td>126</td>\n",
       "      <td>36</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[ess_count 4\\n, prog_count 4\\n, rit_count 50]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          counts_text ess_count prog_count  \\\n",
       "0       [ess_count 4\\n, prog_count 1\\n, rit_count 29]         4          1   \n",
       "1   [ess_count 45\\n, prog_count 116\\n, rit_count 1...        45        116   \n",
       "2       [ess_count 3\\n, prog_count 7\\n, rit_count 80]         3          7   \n",
       "3    [ess_count 93\\n, prog_count 85\\n, rit_count 975]        93         85   \n",
       "4       [ess_count 4\\n, prog_count 1\\n, rit_count 35]         4          1   \n",
       "5       [ess_count 1\\n, prog_count 8\\n, rit_count 25]         1          8   \n",
       "6    [ess_count 50\\n, prog_count 17\\n, rit_count 167]        50         17   \n",
       "7   [ess_count 129\\n, prog_count 84\\n, rit_count 763]       129         84   \n",
       "8    [ess_count 89\\n, prog_count 60\\n, rit_count 774]        89         60   \n",
       "9       [ess_count 4\\n, prog_count 1\\n, rit_count 35]         4          1   \n",
       "10  [ess_count 116\\n, prog_count 158\\n, rit_count ...       116        158   \n",
       "11  [ess_count 31\\n, prog_count 120\\n, rit_count 527]        31        120   \n",
       "12      [ess_count 4\\n, prog_count 1\\n, rit_count 35]         4          1   \n",
       "13  [ess_count 36\\n, prog_count 901\\n, rit_count 2...        36        901   \n",
       "14     [ess_count 1\\n, prog_count 7\\n, rit_count 192]         1          7   \n",
       "15  [ess_count 104\\n, prog_count 391\\n, rit_count ...       104        391   \n",
       "16  [ess_count 112\\n, prog_count 120\\n, rit_count ...       112        120   \n",
       "17     [ess_count 10\\n, prog_count 8\\n, rit_count 44]        10          8   \n",
       "18    [ess_count 5\\n, prog_count 12\\n, rit_count 162]         5         12   \n",
       "19     [ess_count 10\\n, prog_count 0\\n, rit_count 40]        10          0   \n",
       "20  [ess_count 131\\n, prog_count 134\\n, rit_count ...       131        134   \n",
       "21  [ess_count 126\\n, prog_count 507\\n, rit_count ...       126        507   \n",
       "22   [ess_count 47\\n, prog_count 85\\n, rit_count 300]        47         85   \n",
       "23  [ess_count 237\\n, prog_count 276\\n, rit_count ...       237        276   \n",
       "24  [ess_count 653\\n, prog_count 489\\n, rit_count ...       653        489   \n",
       "25       [ess_count 0\\n, prog_count 0\\n, rit_count 0]       NaN        NaN   \n",
       "26       [ess_count 0\\n, prog_count 0\\n, rit_count 0]       NaN        NaN   \n",
       "27       [ess_count 0\\n, prog_count 0\\n, rit_count 0]       NaN        NaN   \n",
       "28  [ess_count 126\\n, prog_count 36\\n, rit_count 640]       126         36   \n",
       "29      [ess_count 4\\n, prog_count 4\\n, rit_count 50]         4          4   \n",
       "\n",
       "   rit_count  \n",
       "0         29  \n",
       "1       1096  \n",
       "2         80  \n",
       "3        975  \n",
       "4         35  \n",
       "5         25  \n",
       "6        167  \n",
       "7        763  \n",
       "8        774  \n",
       "9         35  \n",
       "10      1067  \n",
       "11       527  \n",
       "12        35  \n",
       "13      2406  \n",
       "14       192  \n",
       "15      1520  \n",
       "16      2326  \n",
       "17        44  \n",
       "18       162  \n",
       "19        40  \n",
       "20      2556  \n",
       "21      2200  \n",
       "22       300  \n",
       "23      3233  \n",
       "24      5641  \n",
       "25       NaN  \n",
       "26       NaN  \n",
       "27       NaN  \n",
       "28       640  \n",
       "29        50  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfslice[\"counts_text\"] = dfslice.counts_file.progress_apply(lambda x: load_list(\"{}\".format(str(x))))\n",
    "dfslice.loc[downloaded,\"ess_count\"] = dfslice.loc[downloaded,\"counts_text\"].progress_apply(lambda x: \"{}\".format(str(x[0].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 1st row in counts_text: take as uint dtype\n",
    "dfslice.loc[downloaded,\"prog_count\"] = dfslice.loc[downloaded,\"counts_text\"].progress_apply(lambda x: \"{}\".format(str(x[1].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 2nd row\n",
    "dfslice.loc[downloaded,\"rit_count\"] = dfslice.loc[downloaded,\"counts_text\"].progress_apply(lambda x: \"{}\".format(str(x[2].split()[-1]))).apply(pd.to_numeric,downcast='unsigned') # 2nd element of 3rd row\n",
    "dfslice[[\"counts_text\",\"ess_count\",\"prog_count\",\"rit_count\"]]\n",
    "#df[downloaded][\"ess_strength\"] = df[downloaded][\"ess_count\"]/df[downloaded].rit_count\n",
    "#df[downloaded][\"prog_strength\"] = df[downloaded][\"prog_count\"]/df[downloaded].rit_count\n",
    "#print(df[downloaded]['prog_strength'])\n",
    "#print(\"Got Here M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfslice.loc[0,\"rit_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wget_fail_flag</th>\n",
       "      <th>webtext</th>\n",
       "      <th>ess_count</th>\n",
       "      <th>rit_count</th>\n",
       "      <th>prog_count</th>\n",
       "      <th>counts_text</th>\n",
       "      <th>html_file_count</th>\n",
       "      <th>error_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to main content\\n, State of Alaska\\n, my...</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 29]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>45</td>\n",
       "      <td>1096</td>\n",
       "      <td>116</td>\n",
       "      <td>[ess_count 45\\n, prog_count 116\\n, rit_count 1...</td>\n",
       "      <td>61</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Tongass School of Arts and Sciences\\n, Home\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>[ess_count 3\\n, prog_count 7\\n, rit_count 80]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[Main\\n, Main\\n, Anchorage school district\\n, ...</td>\n",
       "      <td>93</td>\n",
       "      <td>975</td>\n",
       "      <td>85</td>\n",
       "      <td>[ess_count 93\\n, prog_count 85\\n, rit_count 975]</td>\n",
       "      <td>49</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to main content\\n, State of Alaska\\n, my...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[Menu\\n, Home\\n, About\\n, Mission\\n, Leadershi...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>[ess_count 1\\n, prog_count 8\\n, rit_count 25]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[Main\\n, Anchorage School District\\n, Educatin...</td>\n",
       "      <td>50</td>\n",
       "      <td>167</td>\n",
       "      <td>17</td>\n",
       "      <td>[ess_count 50\\n, prog_count 17\\n, rit_count 167]</td>\n",
       "      <td>5</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[Home\\n, APC\\n, Business\\n, Forms\\n, Handbooks...</td>\n",
       "      <td>129</td>\n",
       "      <td>763</td>\n",
       "      <td>84</td>\n",
       "      <td>[ess_count 129\\n, prog_count 84\\n, rit_count 763]</td>\n",
       "      <td>25</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[Main\\n, Main\\n, Anchorage school district\\n, ...</td>\n",
       "      <td>89</td>\n",
       "      <td>774</td>\n",
       "      <td>60</td>\n",
       "      <td>[ess_count 89\\n, prog_count 60\\n, rit_count 774]</td>\n",
       "      <td>33</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to main content\\n, State of Alaska\\n, my...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[Main\\n, Anchorage school district\\n, ASD Depa...</td>\n",
       "      <td>116</td>\n",
       "      <td>1067</td>\n",
       "      <td>158</td>\n",
       "      <td>[ess_count 116\\n, prog_count 158\\n, rit_count ...</td>\n",
       "      <td>35</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[Juneau Community Charter School \\n, JCCS \\n, ...</td>\n",
       "      <td>31</td>\n",
       "      <td>527</td>\n",
       "      <td>120</td>\n",
       "      <td>[ess_count 31\\n, prog_count 120\\n, rit_count 527]</td>\n",
       "      <td>11</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to main content\\n, State of Alaska\\n, my...</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>[ess_count 4\\n, prog_count 1\\n, rit_count 35]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[Fireweed Academy\\n, 995 Soundview Ave. Homer,...</td>\n",
       "      <td>36</td>\n",
       "      <td>2406</td>\n",
       "      <td>901</td>\n",
       "      <td>[ess_count 36\\n, prog_count 901\\n, rit_count 2...</td>\n",
       "      <td>273</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[Soldotna Elementary School\\n, 162 Park Ave. S...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>7</td>\n",
       "      <td>[ess_count 1\\n, prog_count 7\\n, rit_count 192]</td>\n",
       "      <td>23</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[Kaleidoscope School of Arts and Science\\n, 54...</td>\n",
       "      <td>104</td>\n",
       "      <td>1520</td>\n",
       "      <td>391</td>\n",
       "      <td>[ess_count 104\\n, prog_count 391\\n, rit_count ...</td>\n",
       "      <td>87</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>112</td>\n",
       "      <td>2326</td>\n",
       "      <td>120</td>\n",
       "      <td>[ess_count 112\\n, prog_count 120\\n, rit_count ...</td>\n",
       "      <td>61</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>[Home\\n, Classroom Pages \\n, ~ Mrs. MacDonald\\...</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>[ess_count 10\\n, prog_count 8\\n, rit_count 44]</td>\n",
       "      <td>3</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>12</td>\n",
       "      <td>[ess_count 5\\n, prog_count 12\\n, rit_count 162]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[\u001e",
       "Å§\u0005Jù ̈ï ́&lt;Ö\u0002 ̧(­ðíwö(Z.`lÄa\u0000\u001d",
       "Ã*âGz\u000e&lt;ÏN\u001aQÐ...</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>[ess_count 10\\n, prog_count 0\\n, rit_count 40]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>131</td>\n",
       "      <td>2556</td>\n",
       "      <td>134</td>\n",
       "      <td>[ess_count 131\\n, prog_count 134\\n, rit_count ...</td>\n",
       "      <td>73</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to content\\n, About\\n, Mission\\n, Histor...</td>\n",
       "      <td>126</td>\n",
       "      <td>2200</td>\n",
       "      <td>507</td>\n",
       "      <td>[ess_count 126\\n, prog_count 507\\n, rit_count ...</td>\n",
       "      <td>85</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>[Home\\n, About Us\\n, Program Description\\n, Sc...</td>\n",
       "      <td>47</td>\n",
       "      <td>300</td>\n",
       "      <td>85</td>\n",
       "      <td>[ess_count 47\\n, prog_count 85\\n, rit_count 300]</td>\n",
       "      <td>31</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>[Ø'TVÒ8n°èM¤·ÿ\u0007Í1\u0002C&lt;\u00031n\u0015sU0&lt;Q\u0002\u000b",
       "èæLÌ\u000fëc-3Î...</td>\n",
       "      <td>237</td>\n",
       "      <td>3233</td>\n",
       "      <td>276</td>\n",
       "      <td>[ess_count 237\\n, prog_count 276\\n, rit_count ...</td>\n",
       "      <td>105</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>653</td>\n",
       "      <td>5641</td>\n",
       "      <td>489</td>\n",
       "      <td>[ess_count 653\\n, prog_count 489\\n, rit_count ...</td>\n",
       "      <td>177</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ess_count 0\\n, prog_count 0\\n, rit_count 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>[\u000f ̄ö-`ùÅ+KÝZ$QÙEP3⁄43ÇÂ1⁄2\u0014Z@ý\u0002a?éGoÌy6?~ ́...</td>\n",
       "      <td>126</td>\n",
       "      <td>640</td>\n",
       "      <td>36</td>\n",
       "      <td>[ess_count 126\\n, prog_count 36\\n, rit_count 640]</td>\n",
       "      <td>53</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>[Skip to Main Content\\n, District Home\\n, Sele...</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>[ess_count 4\\n, prog_count 4\\n, rit_count 50]</td>\n",
       "      <td>1</td>\n",
       "      <td>[duplicate_flag 0\\n, parse_error_flag 0\\n, wge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wget_fail_flag                                            webtext  \\\n",
       "0               0  [Skip to main content\\n, State of Alaska\\n, my...   \n",
       "1               0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "2               0  [Tongass School of Arts and Sciences\\n, Home\\n...   \n",
       "3               0  [Main\\n, Main\\n, Anchorage school district\\n, ...   \n",
       "4               0  [Skip to main content\\n, State of Alaska\\n, my...   \n",
       "5               0  [Menu\\n, Home\\n, About\\n, Mission\\n, Leadershi...   \n",
       "6               0  [Main\\n, Anchorage School District\\n, Educatin...   \n",
       "7               0  [Home\\n, APC\\n, Business\\n, Forms\\n, Handbooks...   \n",
       "8               0  [Main\\n, Main\\n, Anchorage school district\\n, ...   \n",
       "9               0  [Skip to main content\\n, State of Alaska\\n, my...   \n",
       "10              0  [Main\\n, Anchorage school district\\n, ASD Depa...   \n",
       "11              0  [Juneau Community Charter School \\n, JCCS \\n, ...   \n",
       "12              0  [Skip to main content\\n, State of Alaska\\n, my...   \n",
       "13              0  [Fireweed Academy\\n, 995 Soundview Ave. Homer,...   \n",
       "14              0  [Soldotna Elementary School\\n, 162 Park Ave. S...   \n",
       "15              0  [Kaleidoscope School of Arts and Science\\n, 54...   \n",
       "16              0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "17              0  [Home\\n, Classroom Pages \\n, ~ Mrs. MacDonald\\...   \n",
       "18              0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "19              0  [\n",
       "Å§\u0005Jù ̈ï ́<Ö\u0002 ̧(­ðíwö(Z.`lÄa\u0000\n",
       "Ã*âGz\u000e<ÏN\u001aQÐ...   \n",
       "20              0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "21              0  [Skip to content\\n, About\\n, Mission\\n, Histor...   \n",
       "22              0  [Home\\n, About Us\\n, Program Description\\n, Sc...   \n",
       "23              0  [Ø'TVÒ8n°èM¤·ÿ\u0007Í1\u0002C<\u00031n\u0015sU0<Q\u0002\n",
       "èæLÌ\u000fëc-3Î...   \n",
       "24              0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "25              1                                                      \n",
       "26              1                                                      \n",
       "27              1                                                      \n",
       "28              0  [\u000f ̄ö-`ùÅ+KÝZ$QÙEP3⁄43ÇÂ1⁄2\u0014Z@ý\u0002a?éGoÌy6?~ ́...   \n",
       "29              0  [Skip to Main Content\\n, District Home\\n, Sele...   \n",
       "\n",
       "   ess_count rit_count prog_count  \\\n",
       "0          4        29          1   \n",
       "1         45      1096        116   \n",
       "2          3        80          7   \n",
       "3         93       975         85   \n",
       "4          4        35          1   \n",
       "5          1        25          8   \n",
       "6         50       167         17   \n",
       "7        129       763         84   \n",
       "8         89       774         60   \n",
       "9          4        35          1   \n",
       "10       116      1067        158   \n",
       "11        31       527        120   \n",
       "12         4        35          1   \n",
       "13        36      2406        901   \n",
       "14         1       192          7   \n",
       "15       104      1520        391   \n",
       "16       112      2326        120   \n",
       "17        10        44          8   \n",
       "18         5       162         12   \n",
       "19        10        40          0   \n",
       "20       131      2556        134   \n",
       "21       126      2200        507   \n",
       "22        47       300         85   \n",
       "23       237      3233        276   \n",
       "24       653      5641        489   \n",
       "25       NaN       NaN        NaN   \n",
       "26       NaN       NaN        NaN   \n",
       "27       NaN       NaN        NaN   \n",
       "28       126       640         36   \n",
       "29         4        50          4   \n",
       "\n",
       "                                          counts_text html_file_count  \\\n",
       "0       [ess_count 4\\n, prog_count 1\\n, rit_count 29]               1   \n",
       "1   [ess_count 45\\n, prog_count 116\\n, rit_count 1...              61   \n",
       "2       [ess_count 3\\n, prog_count 7\\n, rit_count 80]               1   \n",
       "3    [ess_count 93\\n, prog_count 85\\n, rit_count 975]              49   \n",
       "4       [ess_count 4\\n, prog_count 1\\n, rit_count 35]               1   \n",
       "5       [ess_count 1\\n, prog_count 8\\n, rit_count 25]               1   \n",
       "6    [ess_count 50\\n, prog_count 17\\n, rit_count 167]               5   \n",
       "7   [ess_count 129\\n, prog_count 84\\n, rit_count 763]              25   \n",
       "8    [ess_count 89\\n, prog_count 60\\n, rit_count 774]              33   \n",
       "9       [ess_count 4\\n, prog_count 1\\n, rit_count 35]               1   \n",
       "10  [ess_count 116\\n, prog_count 158\\n, rit_count ...              35   \n",
       "11  [ess_count 31\\n, prog_count 120\\n, rit_count 527]              11   \n",
       "12      [ess_count 4\\n, prog_count 1\\n, rit_count 35]               1   \n",
       "13  [ess_count 36\\n, prog_count 901\\n, rit_count 2...             273   \n",
       "14     [ess_count 1\\n, prog_count 7\\n, rit_count 192]              23   \n",
       "15  [ess_count 104\\n, prog_count 391\\n, rit_count ...              87   \n",
       "16  [ess_count 112\\n, prog_count 120\\n, rit_count ...              61   \n",
       "17     [ess_count 10\\n, prog_count 8\\n, rit_count 44]               3   \n",
       "18    [ess_count 5\\n, prog_count 12\\n, rit_count 162]               1   \n",
       "19     [ess_count 10\\n, prog_count 0\\n, rit_count 40]               1   \n",
       "20  [ess_count 131\\n, prog_count 134\\n, rit_count ...              73   \n",
       "21  [ess_count 126\\n, prog_count 507\\n, rit_count ...              85   \n",
       "22   [ess_count 47\\n, prog_count 85\\n, rit_count 300]              31   \n",
       "23  [ess_count 237\\n, prog_count 276\\n, rit_count ...             105   \n",
       "24  [ess_count 653\\n, prog_count 489\\n, rit_count ...             177   \n",
       "25       [ess_count 0\\n, prog_count 0\\n, rit_count 0]               0   \n",
       "26       [ess_count 0\\n, prog_count 0\\n, rit_count 0]               0   \n",
       "27       [ess_count 0\\n, prog_count 0\\n, rit_count 0]               0   \n",
       "28  [ess_count 126\\n, prog_count 36\\n, rit_count 640]              53   \n",
       "29      [ess_count 4\\n, prog_count 4\\n, rit_count 50]               1   \n",
       "\n",
       "                                           error_text  \n",
       "0   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "1   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "2   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "3   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "4   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "5   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "6   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "7   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "8   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "9   [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "10  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "11  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "12  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "13  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "14  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "15  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "16  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "17  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "18  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "19  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "20  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "21  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "22  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "23  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "24  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "25  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "26  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "27  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "28  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  \n",
       "29  [duplicate_flag 0\\n, parse_error_flag 0\\n, wge...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfslice.loc[:,['wget_fail_flag','webtext','ess_count','rit_count','prog_count','counts_text','html_file_count','error_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'> \n",
      "     ess_strength  prog_strength\n",
      "0       0.137931       0.034483\n",
      "1       0.041058       0.105839\n",
      "2       0.037500       0.087500\n",
      "3       0.095385       0.087179\n",
      "4       0.114286       0.028571\n",
      "5       0.040000       0.320000\n",
      "6       0.299401       0.101796\n",
      "7       0.169069       0.110092\n",
      "8       0.114987       0.077519\n",
      "9       0.114286       0.028571\n",
      "10      0.108716       0.148079\n",
      "11      0.058824       0.227704\n",
      "12      0.114286       0.028571\n",
      "13      0.014963       0.374480\n",
      "14      0.005208       0.036458\n",
      "15      0.068421       0.257237\n",
      "16      0.048151       0.051591\n",
      "17      0.227273       0.181818\n",
      "18      0.030864       0.074074\n",
      "19      0.250000       0.000000\n",
      "20      0.051252       0.052426\n",
      "21      0.057273       0.230455\n",
      "22      0.156667       0.283333\n",
      "23      0.073307       0.085370\n",
      "24      0.115760       0.086687\n",
      "28      0.196875       0.056250\n",
      "29      0.080000       0.080000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "dfslice.loc[downloaded,\"ess_strength\"] = (dfslice.loc[downloaded,\"ess_count\"]/dfslice.loc[downloaded,\"rit_count\"]).apply(pd.to_numeric,downcast='float')\n",
    "dfslice.loc[downloaded,\"prog_strength\"] = (dfslice.loc[downloaded,\"prog_count\"]/dfslice.loc[downloaded,\"rit_count\"]).apply(pd.to_numeric,downcast='float')\n",
    "print(type(dfslice.loc[0,\"ess_strength\"]), \"\\n\", dfslice.loc[downloaded,[\"ess_strength\", \"prog_strength\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "Loading DF: 100%|██████████| 10/10 [00:00<00:00, 11.26it/s]\n",
      "Loading DF: 100%|██████████| 10/10 [00:00<00:00, 78.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got here 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading DF: 100%|██████████| 10/10 [00:00<00:00, 924.98it/s]\n",
      "Loading DF:  50%|█████     | 5/10 [00:00<00:00, 40.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got here 2\n",
      "Got here 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DF: 100%|██████████| 10/10 [00:00<00:00, 44.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got here 4\n",
      "Got here 5\n",
      "Got here 6\n",
      "Got here 7\n",
      "Got Here 8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not TypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    651\u001b[0m             result = expressions.evaluate(op, str_rep, x, y,\n\u001b[0;32m--> 652\u001b[0;31m                                           raise_on_error=True, **eval_kwargs)\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, raise_on_error, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m         return _evaluate(op, op_str, a, b, raise_on_error=raise_on_error,\n\u001b[0;32m--> 210\u001b[0;31m                          **eval_kwargs)\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, raise_on_error, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, raise_on_error, **eval_kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ce314dcde17f>\u001b[0m in \u001b[0;36mpandify_webtext\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got Here 8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ess_strength\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ess_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rit_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prog_strength\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prog_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdownloaded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rit_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         return construct_result(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m    685\u001b[0m                     return _algos.arrmap_object(lvalues,\n\u001b[0;32m--> 686\u001b[0;31m                                                 lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m    687\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/algos_common_helper.pxi\u001b[0m in \u001b[0;36mpandas.algos.arrmap_object (pandas/algos.c:46681)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    685\u001b[0m                     return _algos.arrmap_object(lvalues,\n\u001b[0;32m--> 686\u001b[0;31m                                                 lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m    687\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-28d47e39037e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mthisslice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschooldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpandify_webtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthisslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthisslice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-ce314dcde17f>\u001b[0m in \u001b[0;36mpandify_webtext\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR! Pandify function failed to load parsing output into DataFrame.\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    ERROR! Pandify function failed to load parsing output into DataFrame.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not TypeError"
     ]
    }
   ],
   "source": [
    "thisslice = schooldf.iloc[0:10,:]\n",
    "pandify_webtext(thisslice)\n",
    "thisslice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = schooldf.iloc[0:10,:]\n",
    "#df[\"webtext\"], df[\"keywords_text\"], df[\"ideology_text\"], df[\"ess_count\"], df[\"prog_count\"], df[\"rit_count\"] = df[\"webtext\"].astype(object), df[\"keywords_text\"].astype(object), df[\"ideology_text\"].astype(object), df[\"ess_count\"].astype(int), df[\"prog_count\"].astype(int), df[\"rit_count\"].astype(int)\n",
    "df[\"webtext\"], df[\"keywords_text\"] = [], []\n",
    "df[\"webtext\"] = df[\"webtext\"].astype(object)\n",
    "df[\"keywords_text\"] = df[\"keywords_text\"].astype(object)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Run parsing algorithm on schools (requires access to webcrawl output)\n",
    "\n",
    "if Debug:\n",
    "    test_dicts = dicts_list[:1] # Limit number of schools to test/refine methods\n",
    "    for school in test_dicts:\n",
    "        parse_school(school)\n",
    "    dictfile = \"testing_dicts_\" + str(datetime.today())\n",
    "    save_datafile(test_dicts, temp_dir+dictfile, \"JSON\")\n",
    "    sys.exit()\n",
    "                \n",
    "# Use multiprocessing.Pool(numcpus) to run parse_school(),\n",
    "# which parses downloaded webtext and saves the results to local storage:\n",
    "if __name__ == '__main__':\n",
    "    with Pool(numcpus) as p:\n",
    "        p.map(parse_school, tqdm(list(tuplist_zip), desc=\"Parsing folders\"), chunksize=numcpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Load parsing output from disk into analyzable Python object (DataFrame or dicts_list)\n",
    "        \n",
    "\"\"\"# Now use dictify_webtext to load the parsing output from local storage into the list of dictionaries:\n",
    "for school in dicts_list:\n",
    "    try:\n",
    "        school = dictify_webtext(school)\n",
    "    except Exception as e:\n",
    "        print(\"  ERROR! Failed to load into dict parsing output for \" + school[NAME_var])\n",
    "        print(\"  \",e)\n",
    "        school_dict[\"parse_error_flag\"] = 1\n",
    "        continue\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# To work with limited system memory (almost there!), split this df into chunks, compute on each, and recombine later.\n",
    "# The number of chunks equals numcpus, and schooldf is split by numschools/numcpus. \n",
    "for num in range(numcpus):\n",
    "    try:\n",
    "        \"splitdf{}\".format(str(num)) = pd.DataFrame()\n",
    "        dfslice = pd.DataFrame()\n",
    "        dfslice = schooldf.iloc[(splitnum*int(num-1)):(splitnum*int(num)),:]\n",
    "    except Exception as e:\n",
    "        print(\"  ERROR! Script failed to split schooldf into smaller DataFrame #\" + str(num) + \" of \" + str(numcpus) + \".\")\n",
    "        print(\"  \",e)\n",
    "        sys.exit()\n",
    "\n",
    "# Now use pandify_webtext to load the parsing output from local storage into the DataFrame:\n",
    "splitnum = int(round(float(numschools)/numcpus)) # Get chunk number based on total number of schools data\n",
    "names_list = [\"\" for _ in range(numcpus)]\n",
    "for num in numcpus:\n",
    "    try:\n",
    "        names_list[num] = \"splitdf{}\".format(str(num))\n",
    "        dfslice = pd.DataFrame()\n",
    "        dfslice = schooldf.iloc[(splitnum*int(num-1)):(splitnum*int(num)),:]\n",
    "        dfslice = pandify_webtext(dfslice) # Load parsed output into the DF\n",
    "        if num==1:\n",
    "            save_datafile(dfslice,merged_df_file,\"CSV\") # Save this first chunk of results to new file\n",
    "        else:\n",
    "            dfslice.to_csv(merged_df_file,mode=\"a\",columns=False,index=False) # Append this next chunk of results to existing saved results\n",
    "        del dfslice # Free memory by deleting smaller slice\n",
    "        \n",
    "        if num = numcpus:\n",
    "            del schooldf # Free memory by deleting full df, now that all the slices have been taken out\n",
    "        \n",
    "        splitdf1,splitdf2,splitdf3 = [pd.DataFrame() for _ in range(3)] # Initialize DFs to split into\n",
    "        splitnum = int(round(float(numschools)/3)) # Get chunk number based on total number of schools data\n",
    "        splitdf1 = schooldf.iloc[:splitnum,:]\n",
    "        splitdf2 = schooldf.iloc[splitnum:splitnum*2,:]\n",
    "        splitdf3 = schooldf.iloc[splitnum*2:,:]\n",
    "\n",
    "        splitdf1 = pandify_webtext(splitdf1)\n",
    "        save_datafile(splitdf1,merged_df_file,\"CSV\") # Save this first half of results\n",
    "        del splitdf1 # Free memory\n",
    "\n",
    "        splitdf2 = pandify_webtext(splitdf2)\n",
    "        splitdf2.to_csv(merged_df_file,mode=\"a\",columns=False,index=False) # Append these results to existing saved results\n",
    "        del splitdf2 # Free memory\n",
    "\n",
    "        splitdf3 = pandify_webtext(splitdf3)\n",
    "        splitdf3.to_csv(merged_df_file,mode=\"a\",columns=False,index=False) # Append these results to existing saved results\n",
    "        del splitdf3 # Free memory\n",
    "                  \n",
    "    except Exception as e:\n",
    "        print(\"  ERROR! Script failed to load parsing output into DataFrame #\" + str(num) + \" of \" + str(numcpus) + \".\"\")\n",
    "        print(\"  \",e)\n",
    "        sys.exit()\"\"\"\n",
    "\n",
    "\n",
    "# Now create a Pandas DataFrame (from dicts_list or from file) and store the data in a memory-efficient way:\n",
    "schooldf = pd.DataFrame.from_dict(dicts_list) # Convert dicts_list into a DataFrame\n",
    "#schooldf = pd.read_pickle(temp_dir+\"school_dicts_temp.pickle\") # Use existing file while debugging pandify_webtext()\n",
    "#schooldf = pd.read_csv(data_loc) # Creating school_df from scratch\n",
    "schooldf = convert_df(schooldf) # Make this DF memory-efficient by converting appropriate columns to category data type\n",
    "tqdm.pandas(desc=\"Loading DF\") # To show progress, create & register new `tqdm` instance with `pandas`\n",
    "\n",
    "\n",
    "# Load parsing output into big pandas DataFrame through slices (to work with limited system memory):\n",
    "if dicts_list is not None:\n",
    "    del dicts_list # Free memory\n",
    "    \n",
    "merged_df_file = temp_dir+\"mergedf_\"+str(datetime.today().strftime(\"%Y-%m-%d\"))+\".csv\" # Prepare file name\n",
    "slice_pandify(schooldf, numcpus*5, merged_df_file)\n",
    "print(\"Larger DF successfully split into \" + str(numcpus*5) + \" smaller DFs, parsed, combined, and saved to file!\")\n",
    "\n",
    "if schooldf is not None:\n",
    "    del schooldf # Free memory\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "# Save final output:\n",
    "print(\"\\nSCHOOL PARSING COMPLETE!!!\")\n",
    "schooldf = pd.read_csv(merged_df_file,header=190) # Load full DF so we can save it pickle-style\n",
    "schooldf = schooldf[schooldf.ADDRESS14 != 'ADDRESS14'] # Clean out any cases of header being written as row\n",
    "newfile = \"charters_parsed_\" + str(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "save_datafile(schooldf, save_dir+newfile, \"csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
