{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google search using Python\n",
    "\n",
    "### Description and external resources:\n",
    "\n",
    "> This script uses two related functions to scrape the best URL from online sources: \n",
    ">> The Google Places API. See the [GitHub page](https://github.com/slimkrazy/python-google-places) for the Python wrapper and sample code, [Google Web Services](https://developers.google.com/places/web-service/) for general documentation, and [here](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    ">> The Google Search function (manually filtered). See [here](https://pypi.python.org/pypi/google) for source code and [here](http://pythonhosted.org/google/) for documentation.\n",
    "\n",
    "> To get an API key for the Google Places API (or Knowledge Graph API), go to the [Google API Console](http://code.google.com/apis/console).\n",
    "> To upgrade your quota limits, sign up for billing--it's free and raises your daily request quota from 1K to 150K (!!).\n",
    "\n",
    "> The code below doesn't use Google's Knowledge Graph (KG) Search API because this turns out NOT to reveal websites related to search results (despite these being displayed in the KG cards visible at right in a standard Google search). The KG API is only useful for scraping KG id, description, name, and other basic/ irrelevant info. TO see examples of how the KG API constructs a search URL, etc., (see [here](http://searchengineland.com/cool-tricks-hack-googles-knowledge-graph-results-featuring-donald-trump-268231)).\n",
    "\n",
    "> Possibly useful note on debugging: An issue causing the GooglePlaces package to unnecessarily give a \"ValueError\" and stop was resolved in [July 2017](https://github.com/slimkrazy/python-google-places/issues/59).\n",
    "> Other instances of this error may occur if Google Places API cannot identify a location as given. Dealing with this is a matter of proper Exception handling (which seems to be working fine below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Python search environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING KEY PACKAGES\n",
    "from googlesearch import search  # automated Google Search package\n",
    "from googleplaces import GooglePlaces, types, lang  # Google Places API\n",
    "\n",
    "import csv, re, os  # Standard packages\n",
    "import pandas as pd  # for working with csv files\n",
    "import urllib, requests  # for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyDdcpqQCSL5O-ypA_g0-n6cRWHcvrZVQhM\n"
     ]
    }
   ],
   "source": [
    "# Initializing Google Places API search functionality\n",
    "places_api_key = re.sub(\"\\n\", \"\", open(\"../data/places_api_key.txt\").read())\n",
    "print(places_api_key)\n",
    "\n",
    "google_places = GooglePlaces(places_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_to_csv(list_of_dicts, file_name, header):\n",
    "    '''This helper function writes a list of dictionaries to a csv called file_name, with column names decided by 'header'.'''\n",
    "    \n",
    "    with open(file_name, 'w') as output_file:\n",
    "        print(\"Saving to \" + str(file_name) + \" ...\")\n",
    "        dict_writer = csv.DictWriter(output_file, header)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# reloading original data set and adding to it \n",
    "# this resolves problem where 2nd half the entries were deleted, though 1st half had good URLs scraped!\n",
    "\n",
    "df_sample1 = pd.read_csv('../sample.csv')\n",
    "df_full = pd.read_csv('../charter_URLs_Apr17.csv')\n",
    "\n",
    "df_full['URL'] = df_sample1['URL']\n",
    "\n",
    "df_full.to_csv('../sample.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's a list of sites we DON'T want to spider, \n",
    "# but that an automated Google search might return...\n",
    "# and we might thus accidentally spider unless we filter them out (as below)!\n",
    "\n",
    "bad_sites = []\n",
    "with open('../data/bad_sites.csv', 'r', encoding = 'utf-8') as csvfile:\n",
    "    for row in csvfile:\n",
    "        bad_sites.append(re.sub('\\n', '', row))\n",
    "\n",
    "# print(bad_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "River City Scholars\n",
      "http://rivercityscholars.org/\n",
      "944 Evergreen St SE, Grand Rapids, MI 49507, USA\n"
     ]
    }
   ],
   "source": [
    "# See the Google Places API wrapper at work!\n",
    "school_name = \"River City Scholars Charter Academy\"\n",
    "address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "query_result = google_places.nearby_search(\n",
    "        location=address, name=school_name,\n",
    "        radius=15000, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "\n",
    "for place in query_result.places:\n",
    "    print(place.name)\n",
    "    place.get_details()  # makes further API call\n",
    "    #print(place.details) # A dict matching the JSON response from Google.\n",
    "    print(place.website)\n",
    "    print(place.formatted_address)\n",
    "\n",
    "# Are there any additional pages of results?\n",
    "if query_result.has_next_page_token:\n",
    "    query_result_next_page = google_places.nearby_search(\n",
    "            pagetoken=query_result.next_page_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.har.com/school/015806106/dr-david-c-walker-elementary-school\n",
      "https://www.excellence-sa.org/walker\n",
      "https://www.excellence-sa.org/\n",
      "https://www.excellence-sa.org/campuses\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/#Test_scores\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/#Low-income_students\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/#Students_with_Disabilities\n",
      "https://www.schooldigger.com/go/TX/schools/0006211404/school.aspx\n",
      "https://www.facebook.com/pages/Dr-David-C-Walker-Int/598905323548274\n",
      "https://www.lasikplus.com/location/san-antonio-lasik-center\n",
      "https://www.publicschoolreview.com/dr-david-c-walker-intermediate-school-profile\n"
     ]
    }
   ],
   "source": [
    "# Example of using the google search function:\n",
    "for url in search('DR DAVID C WALKER INT 6500 IH 35 N STE C, SAN ANTONIO, TX 78218', \\\n",
    "                  stop=5, pause=5.0):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For reference (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!cat > bad_sites\\n\\n  114  cat > testlist.txt\\n  115  cat testlist.txt \\n  116  for i in $(cat testlist.txt | head -n 4); do echo $i; done\\n  117  for i in $(cat testlist.txt | head -n 4); do echo wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\\n  118  for i in $(cat testlist.txt | head -n 4); do echo wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; echo; echo; done\\n  119  for i in $(cat testlist.txt | head -n 4); do wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\\n  120  ls -la\\n  121  rm -f 500425-s 55362003.pdf franklin-benjamin-charter-school-mesa index.html \\n  122* for i in $(cat testlist.txt | head -n 4); do wget --mirror --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Helpful bash-fu\n",
    "\n",
    "'''\n",
    "!cat > bad_sites\n",
    "\n",
    "  114  cat > testlist.txt\n",
    "  115  cat testlist.txt \n",
    "  116  for i in $(cat testlist.txt | head -n 4); do echo $i; done\n",
    "  117  for i in $(cat testlist.txt | head -n 4); do echo wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\n",
    "  118  for i in $(cat testlist.txt | head -n 4); do echo wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; echo; echo; done\n",
    "  119  for i in $(cat testlist.txt | head -n 4); do wget --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\n",
    "  120  ls -la\n",
    "  121  rm -f 500425-s 55362003.pdf franklin-benjamin-charter-school-mesa index.html \n",
    "  122* for i in $(cat testlist.txt | head -n 4); do wget --mirror --exclude-domains=$(echo $(cat ../Charter-school-identities/bad_sites.txt  ) | tr ' ' ,) $i; done\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reading in ../sample.csv ...\n"
     ]
    }
   ],
   "source": [
    "sample = []  # make empty list in which to store the dictionaries\n",
    "\n",
    "if os.path.exists('../sample.csv'):  # first, check if file containing search results is available on disk\n",
    "    file_path = '../sample.csv'\n",
    "else:  # use original data if no existing results are available on disk\n",
    "    file_path = '../charter_URLs_Apr17.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding = 'utf-8')\\\n",
    "as csvfile: # open file                      \n",
    "    print('  Reading in ' + str(file_path) + ' ...')\n",
    "    reader = csv.DictReader(csvfile)  # create a reader\n",
    "    for row in reader:  # loop through rows\n",
    "        sample.append(row)  # append each row to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new \"URL\" and \"NUM_BAD_URLS\" variables for each school, without overwriting any with data there already:\n",
    "for school in sample:\n",
    "    try:\n",
    "        if school[\"URL\"]:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"URL\"] = \"\"\n",
    "\n",
    "for school in sample:\n",
    "    try:\n",
    "        if school[\"NUM_BAD_URLS\"]:\n",
    "            pass\n",
    "        \n",
    "    except (KeyError, NameError):\n",
    "        school[\"NUM_BAD_URLS\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 schools in this data are missing URLs.\n"
     ]
    }
   ],
   "source": [
    "def count_left(list_of_dicts, varname):\n",
    "    '''This helper function determines how many dicts in list_of_dicts don't have a valid key/value pair with key varname.'''\n",
    "    \n",
    "    count = 0\n",
    "    for school in list_of_dicts:\n",
    "        if school[varname] == \"\" or school[varname] == None:\n",
    "            count += 1\n",
    "\n",
    "    print(str(count) + \" schools in this data are missing \" + str(varname) + \"s.\")\n",
    "\n",
    "count_left(sample, 'URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LITTLETON PREP CHARTER SCHOOL 5301 S. BANNOCK ST., LITTLETON, CO 80120 \n",
      " http://www.wellspringlittleton.com/contact/ \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# CHECK OUT RESULTS\n",
    "# TO DO: Make a histogram of 'NUM_BAD_URLS'\n",
    "# systematic way to look at problem URLs (with k > 0)?\n",
    "\n",
    "f = 0\n",
    "for school in sample:\n",
    "    if int(school['NUM_BAD_URLS']) > 14:\n",
    "        print(school[\"SEARCH\"], \"\\n\", school[\"URL\"], \"\\n\")\n",
    "        f += 1\n",
    "\n",
    "print(str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL 3425 WINTER LK RD LAC1200, WINTER HAVEN, FL 33881 \n",
      " https://www.polk.edu/charter-high-schools/ \n",
      " 3425 WINTER LK RD LAC1200, WINTER HAVEN, FL 33881 \n",
      "\n",
      "dict_keys(['SEARCH', 'NCESSCH', 'ADDRESS', 'STABR', 'NUM_BAD_URLS', 'MANUAL_URL', 'URL', 'OLD_URL', 'SCH_NAME'])\n"
     ]
    }
   ],
   "source": [
    "#### Take a look at the first entry's contents and the variables list in our sample (a list of dictionaries)\n",
    "print(sample[1][\"SEARCH\"], \"\\n\", sample[1][\"OLD_URL\"], \"\\n\", sample[1][\"ADDRESS\"], \"\\n\")\n",
    "print(sample[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getURL(school_name, address, bad_sites_list, manual_url):\n",
    "    \n",
    "    '''This function finds the one best URL for a school using two methods:\n",
    "    \n",
    "    1. If a school with this name can be found within 20 km (to account for proximal relocations) in\n",
    "    the Google Maps database (using the Google Places API), AND\n",
    "    if this school has a website on record, then this website is returned.\n",
    "    If no school is found, the school discovered has missing data in Google's database (latitude/longitude, \n",
    "    address, etc.), or the address on record is unreadable, this passes to method #2. \n",
    "    \n",
    "    2. An automated Google search using the school's name + address. This is an essential backup plan to \n",
    "    Google Places API, because sometimes the address on record (courtesy of Dept. of Ed. and our tax dollars) is not \n",
    "    in Google's database. For example, look at: \"3520 Central Pkwy Ste 143 Mezz, Cincinnati, OH 45223\". \n",
    "    No wonder Google Maps can't find this. How could it intelligibly interpret \"Mezz\"?\n",
    "    \n",
    "    Whether using the first or second method, this function excludes URLs with any of the 62 bad_sites defined above, \n",
    "    e.g. trulia.com, greatschools.org, mapquest. It returns the number of excluded URLs (from either method) \n",
    "    and the first non-bad URL discovered.'''\n",
    "    \n",
    "    \n",
    "    ## INITIALIZE\n",
    "    \n",
    "    new_urls = []    # start with empty list\n",
    "    good_url = \"\"    # output goes here\n",
    "    k = 0    # initialize counter for number of URLs skipped\n",
    "    \n",
    "    radsearch = 15000  # define radius of Google Places API search, in km\n",
    "    numgoo = 20  # define number of google results to collect for method #2\n",
    "    wait_time = 20.0  # define length of pause between Google searches (longer is better for big catches like this)\n",
    "    \n",
    "    search_terms = school_name + \" \" + address\n",
    "    print(\"Getting URL for\", school_name + \", \" + address + \"...\")    # show school name & address\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## FIRST URL-SCRAPE ATTEMPT: GOOGLE PLACES API\n",
    "    # Search for nearest school with this name within radsearch km of this address\n",
    "    \n",
    "    try:\n",
    "        query_result = google_places.nearby_search(\n",
    "            location=address, name=school_name,\n",
    "            radius=radsearch, types=[types.TYPE_SCHOOL], rankby='distance')\n",
    "        \n",
    "        for place in query_result.places:\n",
    "            place.get_details()  # Make further API call to get detailed info on this place\n",
    "\n",
    "            found_name = place.name  # Compare this name in Places API to school's name on file\n",
    "            found_address = place.formatted_address  # Compare this address in Places API to address on file\n",
    "\n",
    "            try: \n",
    "                url = place.website  # Grab school URL from Google Places API, if it's there\n",
    "\n",
    "                if any(domain in url for domain in bad_sites_list):\n",
    "                    k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                    #print(\"  URL in Google Places API is a bad site. Moving on.\")\n",
    "\n",
    "                else:\n",
    "                    good_url = url\n",
    "                    print(\"    Success! URL obtained from Google Places API with \" + str(k) + \" bad URLs avoided.\")\n",
    "                    \n",
    "                    '''\n",
    "                    # For testing/ debugging purposes:\n",
    "                    \n",
    "                    print(\"  VALIDITY CHECK: Is the discovered URL of \" + good_url + \\\n",
    "                          \" consistent with the known URL of \" + manual_url + \" ?\")\n",
    "                    print(\"  Also, is the discovered name + address of \" + found_name + \" \" + found_address + \\\n",
    "                          \" consistent with the known name/address of: \" + search_terms + \" ?\")\n",
    "                    '''\n",
    "                    \n",
    "                    if manual_url != \"\":\n",
    "                        if manual_url == good_url:\n",
    "                            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "                            \n",
    "                    return(k, good_url)  # Returns valid URL of the Place discovered in Google Places API\n",
    "        \n",
    "            except:  # No URL in the Google database? Then try next API result or move on to Google searching.\n",
    "                print(\"  Error collecting URL from Google Places API. Moving on.\")\n",
    "                pass\n",
    "    \n",
    "    except:\n",
    "        print(\"  Google Places API search failed. Moving on to Google search.\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "    ## SECOND URL-SCRAPE ATTEMPT: FILTERED GOOGLE SEARCH\n",
    "    # Automate Google search and take first result that doesn't have a bad_sites_list element in it.\n",
    "    \n",
    "    \n",
    "    # Loop through google search output to find first good result:\n",
    "    try:\n",
    "        new_urls = list(search(search_terms, stop=numgoo, pause=wait_time))  # Grab first numgoo Google results (URLs)\n",
    "        print(\"  Successfully collected Google search results.\")\n",
    "        \n",
    "        for url in new_urls:\n",
    "            if any(domain in url for domain in bad_sites_list):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                #print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "                break    # Exit for loop after first good url is found\n",
    "                \n",
    "    \n",
    "    except:\n",
    "        print(\"  Problem with collecting Google search results. Try this by hand instead.\")\n",
    "            \n",
    "        \n",
    "    '''\n",
    "    # For testing/ debugging purposes:\n",
    "    \n",
    "    if k>2:  # Print warning messages depending on number of bad sites preceding good_url\n",
    "        print(\"  WARNING!! CHECK THIS URL!: \" + good_url + \\\n",
    "              \"\\n\" + str(k) + \" bad Google results have been omitted.\")\n",
    "    if k>1:\n",
    "        print(str(k) + \" bad Google results have been omitted. Check this URL!\")\n",
    "    elif k>0:\n",
    "        print(str(k) + \" bad Google result has been omitted. Check this URL!\")\n",
    "    else: \n",
    "        print(\"  No bad sites detected. Reliable URL!\")\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if manual_url != \"\":\n",
    "        if manual_url == good_url:\n",
    "            print(\"    Awesome! The known and discovered URLs are the SAME!\")\n",
    "    \n",
    "    if good_url == \"\":\n",
    "        print(\"  WARNING! No good URL found via API or google search.\\n\")\n",
    "    \n",
    "    return(k, good_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "numschools = 0  # initialize scraping counter\n",
    "\n",
    "keys = sample[0].keys()  # define keys for writing function\n",
    "fname = \"../sample.csv\"  # define file name for writing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for school in sample[:1]:\n",
    "    print(school[\"URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting URL for Richland Two Charter High, 750 Old Clemson Road, Columbia, SC 29229...\n",
      "    Success! URL obtained from Google Places API with 0 bad URLs avoided.\n",
      "\n",
      "\n",
      "URLs discovered for 2 schools.\n"
     ]
    }
   ],
   "source": [
    "# Now to call the above function and actually scrape these things!\n",
    "for school in sample[:1]: # loop through list of schools\n",
    "    if school[\"URL\"] == \"\":  # if URL is missing, fill that in by scraping\n",
    "        numschools += 1\n",
    "        school[\"NUM_BAD_URLS\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESS\"], bad_sites, school[\"MANUAL_URL\"])\n",
    "    \n",
    "    else:\n",
    "        if school[\"URL\"]:\n",
    "            pass  # If URL exists, don't bother scraping it again\n",
    "\n",
    "        else:  # If URL hasn't been defined, then scrape it!\n",
    "            numschools += 1\n",
    "            school[\"NUM_BAD_URLS\"], school[\"URL\"] = \"\", \"\" # start with empty strings\n",
    "            school[\"NUM_BAD_URLS\"], school[\"URL\"] = getURL(school[\"SCH_NAME\"], school[\"ADDRESS\"], bad_sites, school[\"MANUAL_URL\"])\n",
    "\n",
    "print(\"\\n\\nURLs discovered for \" + str(numschools) + \" schools.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above approach works to get a good URL for 6,677 out of the 6,752 schools in this data set. Not bad! \n",
    "\n",
    "> For some reason, the Google search algorithm (method #2) is less likely to work after passing from the Google Places API.\n",
    "\n",
    "> To fill in for the remaining 75, let's skip the function's layers of code and just call the google search function by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL for GLOBAL VILLAGE ACADEMY - FORT COLLINS 8005 HIGHLAND MEADOWS PARKWAY, FORT COLLINS, CO 80528...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 8 bad URLs avoided.\n",
      "9 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for Graham Primary School 140 E 16th Ave, Columbus, OH 43201...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 0 bad URLs avoided.\n",
      "8 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for GRAND CENTER ARTS ACADEMY HIGH 711 NORTH GRAND AVENUE, ST. LOUIS, MO 63103...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 0 bad URLs avoided.\n",
      "7 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for Great Hearts Academies - Archway Glendale Parcel #200-08-098E, Peoria, AZ 85383...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 2 bad URLs avoided.\n",
      "6 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for Halau Lokahi - A New Century Public Charter School 401 Waiakamilo Rd, Honolulu, HI 96817...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 3 bad URLs avoided.\n",
      "5 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for Hamilton Cnty Math & Science 2675 Civic Center Dr, Cincinnati, OH 45231...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 0 bad URLs avoided.\n",
      "4 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for Hardy Brown College Prep 190 Carousel Mall, San Bernardino, CA 92401...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 4 bad URLs avoided.\n",
      "3 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for HARMONY SCIENCE ACADEMY - GRAND PRAIRIE 1102 NW 7TH ST, GRAND PRAIRIE, TX 75050...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 0 bad URLs avoided.\n",
      "2 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for HARRIS COUNTY JUVENILE DETENTION CENTER 1200 CONGRESS ST STE 6500, HOUSTON, TX 77002...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 1 bad URLs avoided.\n",
      "1 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "Scraping URL for HARRIS MIDDLE 325 PRUITT AVE, SAN ANTONIO, TX 78204...\n",
      "  URLs list collected successfully!\n",
      "    Success! URL obtained by Google search with 5 bad URLs avoided.\n",
      "0 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n",
      "\n",
      "0 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n"
     ]
    }
   ],
   "source": [
    "for school in sample:\n",
    "    if school[\"URL\"] == \"\":\n",
    "        k = 0  # initialize counter for number of URLs skipped\n",
    "        school[\"NUM_BAD_URLS\"] = \"\"\n",
    "\n",
    "        print(\"Scraping URL for \" + school[\"SEARCH\"] + \"...\")\n",
    "        urls_list = list(search(school[\"SEARCH\"], stop=20, pause=10.0))\n",
    "        print(\"  URLs list collected successfully!\")\n",
    "\n",
    "        for url in urls_list:\n",
    "            if any(domain in url for domain in bad_sites):\n",
    "                k+=1    # If this url is in bad_sites_list, add 1 to counter and move on\n",
    "                # print(\"  Bad site detected. Moving on.\")\n",
    "            else:\n",
    "                good_url = url\n",
    "                print(\"    Success! URL obtained by Google search with \" + str(k) + \" bad URLs avoided.\")\n",
    "\n",
    "                school[\"URL\"] = good_url\n",
    "                school[\"NUM_BAD_URLS\"] = k\n",
    "                \n",
    "                count_left(sample, 'URL')\n",
    "                dicts_to_csv(sample, fname, keys)\n",
    "                print()\n",
    "                break    # Exit for loop after first good url is found                               \n",
    "                                           \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6751 schools in this data are missing URLs.\n",
      "Saving to ../sample.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Save sample to file (can continue to load and add to it):\n",
    "count_left(sample, 'URL')\n",
    "dicts_to_csv(sample, fname, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
