{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing & Categorizing HTML from `wget` run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, re, fnmatch # for navigating file trees and working with strings\n",
    "import csv # for reading in CSV files\n",
    "#from glob import glob,iglob # for finding files within nested folders--compare with os.walk\n",
    "import json, pickle # For saving a loading dictionaries, etc. from file with JSON and pickle formats\n",
    "from datetime import datetime # For timestamping files\n",
    "import sys # For working with user input\n",
    "from nltk.stem.porter import PorterStemmer # an approximate method of stemming words\n",
    "stemmer = PorterStemmer()\n",
    "from nltk import word_tokenize, sent_tokenize # widely used text tokenizer\n",
    "import urllib, urllib.request # for testing pages\n",
    "from unicodedata import normalize # for cleaning text by converting unicode character encodings into readable format\n",
    "\n",
    "# Import parser\n",
    "from bs4 import BeautifulSoup # BS reads and parses even poorly/unreliably coded HTML \n",
    "from bs4.element import Comment # helps with detecting inline/junk tags when parsing with BS\n",
    "import lxml # for fast HTML parsing with BS\n",
    "bsparser = \"lxml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set script options\n",
    "\n",
    "Debug = False # Set to \"True\" for extra progress reports while algorithms run\n",
    "notebook = True # Use different file paths depending on whether files are being accessed from shell (False) or within a Jupyter notebook (True)\n",
    "usefile = False # Set to \"True\" if loading from file a dicts_list to add to. Confirms with user input first!\n",
    "workstation = False # If working from office PC\n",
    "\n",
    "if notebook:\n",
    "    usefile = False # Prompting user for input file is only useful in command-line\n",
    "\n",
    "inline_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\",\n",
    "               \"em\", \"kbd\", \"strong\", \"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\",\n",
    "               \"span\", \"sub\", \"sup\"] # this list helps with eliminating junk tags when parsing HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set directories\n",
    "\n",
    "if workstation and notebook:\n",
    "    dir_prefix = \"C:\\\\Users\\\\Jaren\\\\Documents\\\\Charter-school-identities\\\\\"\n",
    "elif notebook:\n",
    "    dir_prefix = \"/home/jovyan/work/\"\n",
    "else:\n",
    "    dir_prefix = \"/vol_b/data/\"\n",
    "\n",
    "example_page = \"https://westlakecharter.com/about/\"\n",
    "example_schoolname = \"TWENTY-FIRST_CENTURY_NM\"\n",
    "\n",
    "if workstation and notebook:\n",
    "    micro_sample13 = dir_prefix + \"data\\\\micro-sample13_coded.csv\" #data location for random micro-sample of 300 US charter schools\n",
    "    full_schooldata = dir_prefix + \"data\\\\charter_URLs_2014.csv\" #data location for 2014 population of US charter schools\n",
    "    example_file = dir_prefix + \"data\\\\example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "    dicts_dir = dir_prefix + \"dicts\\\\\" # Directory in which to find & save dictionary files\n",
    "    save_dir = dir_prefix + \"data\\\\\" # Directory in which to save data files\n",
    "\n",
    "else:\n",
    "    wget_dataloc = dir_prefix + \"wget/parll_wget/\" #data location for schools downloaded with wget in parallel (requires server access)\n",
    "    example_folder = wget_dataloc + \"TWENTY-FIRST_CENTURY_NM/\"\n",
    "    example_file = dir_prefix + \"wget/example_file.html\" #example_folder + \"21stcenturypa.com/wp/default?page_id=27.tmp.html\"\n",
    "\n",
    "    micro_sample13 = dir_prefix + \"Charter-school-identities/data/micro-sample13_coded.csv\" #data location for random micro-sample of 300 US charter schools\n",
    "    full_schooldata = dir_prefix + \"Charter-school-identities/data/charter_URLs_2014.csv\" #data location for 2014 population of US charter schools\n",
    "    dicts_dir = dir_prefix + \"Charter-school-identities/dicts/\" # Directory in which to find & save dictionary files\n",
    "    save_dir = dir_prefix + \"Charter-school-identities/data/\" # Directory in which to save data files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input file, if any\n",
    "if usefile and not notebook:\n",
    "    print(\"\\nWould you like to load from file a list of dictionaries to add to? (Y/N)\")\n",
    "    answer = input()\n",
    "    if answer == \"Y\":\n",
    "        print(\"Please indicate file path for dictionary list file.\")\n",
    "        answer2 = input()\n",
    "        if os.path.exists(answer2):\n",
    "            input_file = answer2\n",
    "            usefile = True\n",
    "        else:\n",
    "            print(\"Invalid file path. Aborting script.\")\n",
    "            sys.exit()\n",
    "\n",
    "    elif answer == \"N\":\n",
    "        print(\"OK! This script will create a new file for this list of dictionaries.\")\n",
    "        usefile = False\n",
    "    \n",
    "    else:\n",
    "        print(\"Response not interpretable. Aborting script.\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define (non-parsing) helper functions\n",
    "\n",
    "def get_vars(data):\n",
    "    \"\"\"Defines variable names based on the data source called.\"\"\"\n",
    "    \n",
    "    if data==full_schooldata:\n",
    "        URL_variable = \"TRUE_URL\"\n",
    "        NAME_variable = \"SCH_NAME\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "    \n",
    "    elif data==micro_sample13:\n",
    "        URL_variable = \"URL\"\n",
    "        NAME_variable = \"SCHNAM\"\n",
    "        ADDR_variable = \"ADDRESS\"\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            print(\"Error processing variables from data file \" + str(data) + \"!\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"ERROR: No data source established!\\n\")\n",
    "    \n",
    "    return(URL_variable,NAME_variable,ADDR_variable)\n",
    "\n",
    "\n",
    "def tag_visible(element):\n",
    "    \"\"\"Returns false if a web element has a non-visible tag, \n",
    "    i.e. one site visitors wouldn't actually read--and thus one we don't want to parse\"\"\"\n",
    "    \n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def webtext_from_files(datalocation):\n",
    "    \"\"\"Concatenate and return a single string from all webtext (with .txt format) in datalocation\"\"\"\n",
    "    \n",
    "    string = \"\"\n",
    "    for root, dirs, files in os.walk(datalocation):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                fileloc = open(datalocation+file, \"r\")\n",
    "                string = string + (fileloc.read())\n",
    "    return string\n",
    "\n",
    "\n",
    "def remove_spaces(file_path):\n",
    "    \"\"\"Remove spaces from text file at file_path\"\"\"\n",
    "    \n",
    "    words = [x for x in open(file_path).read().split() if x != \"\"]\n",
    "    text = \"\"\n",
    "    for word in words:\n",
    "        text += word + \" \"\n",
    "    return text\n",
    "\n",
    "\n",
    "def save_to_file(dicts_list, file, mode):\n",
    "    \"\"\"Saves dicts_list to file using JSON or pickle format (whichever was specified).\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    mode = str(mode)\n",
    "    \n",
    "    try:\n",
    "        if mode.upper()==\"JSON\":\n",
    "            if not file.endswith(\".json\"):\n",
    "                file += \".json\"\n",
    "            with open(file, 'w') as outfile:\n",
    "                json.dump(dicts_list, outfile)\n",
    "                print(\"Dicts saved to \" + file + \" in JSON format!\\n\")\n",
    "\n",
    "        elif mode.lower()==\"pickle\":\n",
    "            if not file.endswith(\".pickle\"):\n",
    "                file += \".pickle\"\n",
    "            with open(file, 'wb') as outfile:\n",
    "                pickle.dump(dicts_list, outfile)\n",
    "                print(\"Dicts saved to \" + file + \" in pickle format!\\n\")\n",
    "\n",
    "        else:\n",
    "            print(\"ERROR! Save failed due to improper arguments. These are: file, object to be saved, and file format to save in.\\n\\\n",
    "                  Specify either 'JSON' or 'pickle' as third argument ('mode' or file format) when calling this function.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "def load_file(file):\n",
    "    \"\"\"Loads dicts_list (or whatever) from file, using either JSON or pickle format. \n",
    "    The created object should be assigned when called.\"\"\"\n",
    "    \n",
    "    file = str(file)\n",
    "    \n",
    "    if file.lower().endswith(\".json\"):\n",
    "        with open(file,'r') as infile:\n",
    "            var = json.load(infile)\n",
    "    \n",
    "    if file.lower().endswith(\".pickle\"):\n",
    "        with open(file,'rb') as infile:\n",
    "            var = pickle.load(infile)\n",
    "        \n",
    "    print(file + \" successfully loaded!\\n\")\n",
    "    return var\n",
    "\n",
    "\n",
    "def load_dict(custom_dict, file_path):\n",
    "    \"\"\"Loads in a dictionary. Adds each entry from the dict at file_path to the defined set custom_dict (the input), \n",
    "    which can also be an existing dictionary. This allows the creation of combined dictionaries!\"\"\"\n",
    "\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            custom_dict.add(stemmer.stem(line.replace(\"\\n\", \"\"))) # Add line after stemming dictionary entries and eliminating newlines\n",
    "            line = file_handler.readline() # Look for anything else in that line, add that too\n",
    "    return custom_dict\n",
    "\n",
    "\n",
    "def list_files(folder_path, extension):\n",
    "    \"\"\"Outputs a list of every file in folder_path or its subdirectories that has a specified extension.\n",
    "    Prepends specified extension with '.' if it doesn't start with it already.\n",
    "    If no extension is specified, it just returns all files in folder_path.\"\"\"\n",
    "    \n",
    "    matches = []\n",
    "    if extension:\n",
    "        extension = str(extension) # Coerce to string, just in case\n",
    "    \n",
    "    if extension and not extension.startswith(\".\"):\n",
    "        extension = \".\" + extension\n",
    "    \n",
    "    for dirpath,dirnames,filenames in os.walk(folder_path):\n",
    "        if extension:\n",
    "            for filename in fnmatch.filter(filenames, \"*\" + extension): # Use extension to filter list of files\n",
    "                matches.append(os.path.join(dirpath,filename))\n",
    "        else:\n",
    "                matches.append(os.path.join(dirpath,filename)) # If no extension, just take all files\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Set parsing keywords\n",
    "\n",
    "keywords = ['values', 'academics', 'skills', 'purpose',\n",
    "                       'direction', 'mission', 'vision', 'vision', 'mission', 'our purpose',\n",
    "                       'our ideals', 'ideals', 'our cause', 'curriculum','curricular',\n",
    "                       'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system',\n",
    "                       'structure','philosophy', 'philosophical', 'beliefs', 'believe',\n",
    "                       'principles', 'creed', 'credo', 'values','moral', 'history', 'our story',\n",
    "                       'the story', 'school story', 'background', 'founding', 'founded',\n",
    "                       'established','establishment', 'our school began', 'we began',\n",
    "                       'doors opened', 'school opened', 'about us', 'our school', 'who we are',\n",
    "                       'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = ['mission','vision', 'vision:', 'mission:', 'our purpose', 'our ideals', 'ideals:', 'our cause', 'cause:', 'goals', 'objective']\n",
    "curriculum_keywords = ['curriculum', 'curricular', 'program', 'method', 'pedagogy', 'pedagogical', 'approach', 'model', 'system', 'structure']\n",
    "philosophy_keywords = ['philosophy', 'philosophical', 'beliefs', 'believe', 'principles', 'creed', 'credo', 'value',  'moral']\n",
    "history_keywords = ['history', 'story','our story', 'the story', 'school story', 'background', 'founding', 'founded', 'established', 'establishment', 'our school began', 'we began', 'doors opened', 'school opened']\n",
    "about_keywords =  ['about us', 'our school', 'who we are', 'overview', 'general information', 'our identity', 'profile', 'highlights']\n",
    "\n",
    "mission_keywords = set(stemmer.stem(word) for word in mission_keywords)\n",
    "curriculum_keywords = set(stemmer.stem(word) for word in curriculum_keywords)\n",
    "philosophy_keywords = set(stemmer.stem(word) for word in philosophy_keywords)\n",
    "history_keywords = set(stemmer.stem(word) for word in history_keywords)\n",
    "about_keywords =  set(stemmer.stem(word) for word in about_keywords)\n",
    "keys_dict = set(stemmer.stem(key) for key in keywords)\n",
    "    \n",
    "if Debug:\n",
    "    print(\"\\nList of keywords:\\n\", list(keys_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use with filtering, create combined dictionary for ideologies:\n",
    "\n",
    "ideol_dict = set()\n",
    "ideol_dict = load_dict(ideol_dict, dicts_dir + \"ess_dict.txt\")\n",
    "ideol_dict = load_dict(ideol_dict, dicts_dir + \"prog_dict.txt\")\n",
    "\n",
    "if Debug:\n",
    "    print(len(ideol_dict), \"entries loaded into the combined ideology dictionary.\")\n",
    "    list_dict = list(ideol_dict)\n",
    "    list_dict.sort(key = lambda x: x.lower())\n",
    "    print(\"First 10 elements of combined ideology dictionary are:\\n\", list_dict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Compare parsing by newlines vs. by HTML tags\n",
    "\n",
    "def parseurl_by_newlines(urlstring):\n",
    "    \"\"\"Uses BS to parse HTML from a given URL and looks for three newlines to separate chunks of text.\"\"\"\n",
    "    \n",
    "    # Read HTML from a given url:\n",
    "    with urllib.request.urlopen(urlstring) as url:\n",
    "        s = url.read()\n",
    "    \n",
    "    # Parse raw text from website body:\n",
    "    soup = BeautifulSoup(s, bsparser)\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    webtext = u\" \".join(t.strip() for t in visible_texts)\n",
    "    \n",
    "    return re.split(r'\\s{3,}', webtext)\n",
    "\n",
    "\n",
    "def parseurl_by_tags(urlstring):\n",
    "    \"\"\"Cleans HTML by removing inline tags, ripping out non-visible tags, \n",
    "    replacing paragraph tags with a random string, and finally using this to separate HTML into chunks.\n",
    "    Reads in HTML from the web using a given website address, urlstring.\"\"\"\n",
    "    \n",
    "    with urllib.request.urlopen(urlstring) as url:\n",
    "        HTML_page = url.read()\n",
    "\n",
    "    random_string = \"\".join(map(chr, os.urandom(75))) # Create random string for tag delimiter\n",
    "    soup = BeautifulSoup(HTML_page, bsparser)\n",
    "    \n",
    "    [s.extract() for s in soup(['style', 'script', 'head', 'title', 'meta', '[document]'])] # Remove non-visible tags\n",
    "    for it in inline_tags:\n",
    "        [s.extract() for s in soup(\"</\" + it + \">\")] # Remove inline tags\n",
    "    \n",
    "    visible_text = soup.getText(random_string).replace(\"\\n\", \"\") # Replace \"p\" tags with random string, eliminate newlines\n",
    "    visible_text = list(elem.replace(\"\\t\",\"\").replace(u'\\xa0', u' ') for elem in visible_text.split(random_string)) # Split text into list using random string while eliminating tabs and unicode; OR: normalize(\"NFKC\", elem) \n",
    "    visible_text = list(filter(lambda vt: vt.split() != [], visible_text)) # Eliminate empty elements\n",
    "    # Consider joining list elements together with newline in between by prepending with: \"\\n\".join\n",
    "    \n",
    "    return(visible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Text chunking accuracy of parsing by tags is superior to parsing by newlines:\n",
    "# Compare each of these with the browser-displayed content of example_page:\n",
    "if Debug:\n",
    "    print(parseurl_by_newlines(example_page),\"\\n\\n\",parseurl_by_tags(example_page))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Define parsing helper functions\n",
    "\n",
    "def parsefile_by_tags(HTML_file):\n",
    "    \n",
    "    \"\"\"Cleans HTML by removing inline tags, ripping out non-visible tags, \n",
    "    replacing paragraph tags with a random string, and finally using this to separate HTML into chunks.\n",
    "    Reads in HTML from storage using a given filename, HTML_file.\"\"\"\n",
    "\n",
    "    random_string = \"\".join(map(chr, os.urandom(75))) # Create random string for tag delimiter\n",
    "    soup = BeautifulSoup(open(HTML_file), bsparser)\n",
    "    \n",
    "    [s.extract() for s in soup(['style', 'script', 'head', 'title', 'meta', '[document]'])] # Remove non-visible tags\n",
    "    for it in inline_tags:\n",
    "        [s.extract() for s in soup(\"</\" + it + \">\")] # Remove inline tags\n",
    "    \n",
    "    visible_text = soup.getText(random_string).replace(\"\\n\", \"\") # Replace \"p\" tags with random string, eliminate newlines\n",
    "    # Split text into list using random string while also eliminating tabs and converting unicode to readable text:\n",
    "    visible_text = list(normalize(\"NFKC\",elem.replace(\"\\t\",\"\")) for elem in visible_text.split(random_string))\n",
    "    visible_text = list(filter(lambda vt: vt.split() != [], visible_text)) # Eliminate empty elements\n",
    "    # Consider joining list elements together with newline in between by prepending with: \"\\n\".join\n",
    "\n",
    "    return(visible_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    example_textlist = parsefile_by_tags(example_file)\n",
    "    print(\"Output of parsefile_by_tags:\\n\\n\", example_textlist, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_page(pagetext_list, keyslist):\n",
    "    \n",
    "    \"\"\"Filters webtext of a given .html page, which is parsed and in list format, to only those strings \n",
    "    within pagetext_list containing an element (word or words) of inputted keyslist. \n",
    "    Returns list filteredtext wherein each element has original case (not coerced to lower-case).\"\"\"\n",
    "    \n",
    "    filteredtext = [] # Initialize empty list to hold strings of page\n",
    "    \n",
    "    for string in pagetext_list:\n",
    "        lowercasestring = str(string).lower() # lower-case string...\n",
    "        dict_list = [key.lower() for key in list(keyslist)] # ...compared with lower-case element of keyslist\n",
    "        for key in dict_list:\n",
    "            if key in lowercasestring and key in lowercasestring.split(' '): # Check that the word is the whole word not part of another one\n",
    "                filteredtext.append(string)\n",
    "\n",
    "    return filteredtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    print(\"Output of filter_keywords_page with keywords:\\n\\n\", filter_dict_page(example_textlist, keys_dict), \"\\n\\n\")\n",
    "    \n",
    "    print(\"Output of filter_keywords_page with ideology words:\\n\\n\", filter_dict_page(example_textlist, ideol_dict), \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_school(school_dict, school_name, school_address, school_URL, datalocation, parsed, numschools):\n",
    "    \n",
    "    \"\"\"This core function parses webtext for a given school, using helper functions to run analyses and then saving multiple outputs to school_dict:\n",
    "    full (partially cleaned) webtext, by parsing webtext of each .html file (removing inline tags, etc.) within school's folder, via parsefile_by_tags();\n",
    "    and all text associated with specific categories by filtering webtext to those with elements from a defined keyword list, via filter_keywords_page().\n",
    "    \n",
    "    For the sake of parsimony and manageable script calls, OTHER similar functions/scripts collect these additional outputs: \n",
    "    parsed webtext, having removed overlapping headers/footers common to multiple pages, via remove_overlaps();\n",
    "    all text associated with specific categories by filtering webtext according to keywords for \n",
    "    mission, curriculum, philosophy, history, and about/general self-description, via categorize_page(); and\n",
    "    contents of those individual pages best matching each of these categories, via find_best_categories.\"\"\"\n",
    "    \n",
    "    global itervar # This allows function to access global itervar counter\n",
    "    itervar+=1\n",
    "    \n",
    "    print(\"Parsing \" + str(school_name) + \", which is school #\" + str(itervar) + \" of \" + str(numschools) + \"...\")\n",
    "    \n",
    "    school_dict[\"webtext\"], school_dict[\"keywords_text\"], school_dict[\"ideology_text\"], school_dict[\"duplicate_flag\"], school_dict[\"parse_error_flag\"], school_dict[\"wget_fail_flag\"] = [], [], [], 0, 0, 0\n",
    "    \n",
    "    folder_name = re.sub(\" \",\"_\",(school_name+\" \"+school_address[-8:-6]))\n",
    "    school_dict[\"folder_name\"] = folder_name\n",
    "    \n",
    "    school_folder = datalocation + folder_name + \"/\"\n",
    "\n",
    "    # Check if folder exists. If not, exit function\n",
    "    if not (os.path.exists(school_folder) or os.path.exists(school_folder.lower()) or os.path.exists(school_folder.upper())):\n",
    "        print(\"!! NO DIRECTORY FOUND matching \" + str(school_folder) + \".\\n  Aborting parsing function...\\n\\n\")\n",
    "        school_dict['wget_fail_flag'] = 1\n",
    "        return\n",
    "    \n",
    "    if school_URL not in parsed: #check if this URL has already been parsed. If so, skip this school to avoid duplication bias\n",
    "        parsed.append(school_URL)\n",
    "        \n",
    "        try:\n",
    "            file_count = 0 # initialize count of files parsed\n",
    "            \n",
    "            # Parse file only if it contains HTML. This is easy: use the \"*.html\" wildcard pattern--\n",
    "            # also wget gave the \".html\" file extension to appropriate files when downloading (`--adjust-extension` option)\n",
    "            # Less efficient ways to check if files contain HTML (e.g., for data not downloaded by wget):\n",
    "            # if bool(BeautifulSoup(open(fname), bsparser).find())==True: # if file.endswith(\".html\"):\n",
    "            # Another way to do this, maybe faster but broken: files_iter = iglob(school_folder + \"**/*.html\", recursive=True)\n",
    "            \n",
    "            file_list = list_files(school_folder, \".html\")\n",
    "            \n",
    "            if file_list==(None or school_folder) or not file_list:\n",
    "                print(\"ERROR! File gathering function broken!\\n  Aborting parser for \" + str(school_name) + \"...\")\n",
    "                return\n",
    "            \n",
    "            elif file_list==(\"\" or []):\n",
    "                print(\"  No .html files found.\\n  Aborting parser for \" + str(school_name) + \"...\")\n",
    "                return\n",
    "            \n",
    "            for file in file_list:\n",
    "                                    \n",
    "                file_count+=1 # add to count of parsed files\n",
    "                if Debug:\n",
    "                    print(\"    Parsing HTML in \" + str(file) + \"...\")\n",
    "                    \n",
    "                try:                    \n",
    "                    parsed_pagetext = parsefile_by_tags(file) # Parse page text (filter too?)\n",
    "                        \n",
    "                    school_dict[\"webtext\"].extend(parsed_pagetext) # Add new parsed text to long list\n",
    "\n",
    "                    #school_dict[\"keywords_text\"].extend(filter_dict_page(parsed_pagetext, keys_dict)) # Filter parsed file using keywords list\n",
    "                    #school_dict[\"ideology_text\"].extend(filter_dict_page(parsed_pagetext, ideol_dict)) # Filter parsed file using keywords list\n",
    "\n",
    "                    if Debug:\n",
    "                        print(\"      Successfully parsed and filtered file \" + str(file) + \"...\")\n",
    "                        \n",
    "                    file_count+=1\n",
    "                        \n",
    "                    continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    if Debug:\n",
    "                        print(\"      ERROR! Failed to parse file...\")\n",
    "                        print(\"      \",e)\n",
    "                        continue\n",
    "                    else:\n",
    "                        continue\n",
    "            \n",
    "            print(\"  Parsed page text for \" + str(file_count-1) + \" .html file(s) belonging to \" + str(school_name) + \"...\")\n",
    "            school_dict[\"html_file_count\"] = int(file_count-1)\n",
    "            \n",
    "            print(\"  SUCCESS! Parsed and categorized website text for \" + str(school_name) + \"...\\n\")\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"    ERROR! Failed to parse & categorize webtext of \" + str(school_name))\n",
    "            print(\"    \",e)\n",
    "            school_dict[\"parse_error_flag\"] = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"DUPLICATE URL DETECTED. Skipping \" + str(school_name) + \"...\\n\\n\")\n",
    "        school_dict[\"duplicate_flag\"] = 1\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Preparing data to be parsed\n",
    "\n",
    "itervar = 0 # initialize iterator that counts number of schools already parsed\n",
    "parsed = [] # initialize list of URLs that have already been parsed\n",
    "dicts_list = [] # initialize list of dictionaries to hold school data\n",
    "\n",
    "# If input_file was defined by user input in beginning of script, use that to load list of dictionaries. We'll add to it!\n",
    "if usefile and not dicts_list:\n",
    "    dicts_list = load_file(input_file)\n",
    "    data_loc = full_schooldata # If loading data, assume we're running on full charter population\n",
    "\n",
    "else:\n",
    "    # set charter school data file and corresponding varnames:\n",
    "    \n",
    "    data_loc = full_schooldata # Run at scale using URL list of full charter population\n",
    "    # data_loc = micro_sample13 # This seems nice for debugging--except directories don't match because different data source\n",
    "        \n",
    "    # Create dict list from CSV on file, with one dict per school\n",
    "    with open(data_loc, 'r', encoding = 'Latin1') as csvfile: # open data file\n",
    "        reader = csv.DictReader(csvfile) # create a reader\n",
    "        for row in reader: # loop through rows\n",
    "            dicts_list.append(row) # append each row to the list\n",
    "        \n",
    "URL_var,NAME_var,ADDR_var = get_vars(data_loc) # get varnames depending on data source\n",
    "        \n",
    "# Note on data structures: each row, dicts_list[i] is a dictionary with keys as column name and value as info.\n",
    "# This will be translated into pandas data frame once (rather messy) website text is parsed into consistent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Run parsing algorithm on schools (requires access to webcrawl output)\n",
    "\n",
    "test_dicts = dicts_list[:1] # Limit number of schools to analyze, in order to refine methods\n",
    "\n",
    "if Debug:\n",
    "    for school in test_dicts:\n",
    "        parse_school(school, school[NAME_var], school[ADDR_var], school[URL_var], wget_dataloc, parsed, len(dicts_list))\n",
    "        \n",
    "else:\n",
    "    for school in dicts_list:\n",
    "        parse_school(school, school[NAME_var], school[ADDR_var], school[URL_var], wget_dataloc, parsed, len(dicts_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check out results:\n",
    "if Debug:\n",
    "    print(test_dicts[0])\n",
    "else:\n",
    "    print(dicts_list[7][\"webtext\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output:\n",
    "if Debug:\n",
    "    dictfile = \"testing_dicts_\" + str(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "    save_to_file(test_dicts, save_dir+dictfile, \"JSON\")\n",
    "else:\n",
    "    dictfile = \"school_dicts_\" + str(datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "    save_to_file(dicts_list, save_dir+dictfile, \"JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
